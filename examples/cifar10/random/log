I0315 05:39:09.447758  4128 caffe.cpp:218] Using GPUs 2
I0315 05:39:09.467877  4128 caffe.cpp:223] GPU 2: TITAN X (Pascal)
I0315 05:39:09.838603  4128 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1
test_interval: 1000
base_lr: 0.0005
display: 200
max_iter: 56000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 1000
snapshot_prefix: "examples/cifar10/random/models/"
solver_mode: GPU
device_id: 2
net: "examples/cifar10/random/cifar10_full_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_format: HDF5
I0315 05:39:09.838721  4128 solver.cpp:87] Creating training net from net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:39:09.838927  4128 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0315 05:39:09.838937  4128 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0315 05:39:09.839025  4128 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar1"
  type: "Data"
  top: "data_fixed"
  top: "label_fixed"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_20000"
    batch_size: 80
    backend: LMDB
  }
}
layer {
  name: "cifar2"
  type: "Data"
  top: "data_random"
  top: "label_random"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_random_10000"
    batch_size: 20
    backend: LMDB
  }
}
layer {
  name: "concat_label"
  type: "Concat"
  bottom: "label_fixed"
  bottom: "label_random"
  top: "label"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "concat_data"
  type: "Concat"
  bottom: "data_fixed"
  bottom: "data_random"
  top: "data"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:39:09.839095  4128 layer_factory.hpp:77] Creating layer cifar1
I0315 05:39:09.839172  4128 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_20000
I0315 05:39:09.839205  4128 net.cpp:86] Creating Layer cifar1
I0315 05:39:09.839211  4128 net.cpp:382] cifar1 -> data_fixed
I0315 05:39:09.839228  4128 net.cpp:382] cifar1 -> label_fixed
I0315 05:39:09.839238  4128 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:39:09.841012  4128 data_layer.cpp:45] output data size: 80,3,32,32
I0315 05:39:09.844761  4128 net.cpp:124] Setting up cifar1
I0315 05:39:09.844784  4128 net.cpp:131] Top shape: 80 3 32 32 (245760)
I0315 05:39:09.844787  4128 net.cpp:131] Top shape: 80 (80)
I0315 05:39:09.844789  4128 net.cpp:139] Memory required for data: 983360
I0315 05:39:09.844797  4128 layer_factory.hpp:77] Creating layer cifar2
I0315 05:39:09.844871  4128 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_random_10000
I0315 05:39:09.844887  4128 net.cpp:86] Creating Layer cifar2
I0315 05:39:09.844893  4128 net.cpp:382] cifar2 -> data_random
I0315 05:39:09.844903  4128 net.cpp:382] cifar2 -> label_random
I0315 05:39:09.844909  4128 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:39:09.845031  4128 data_layer.cpp:45] output data size: 20,3,32,32
I0315 05:39:09.846967  4128 net.cpp:124] Setting up cifar2
I0315 05:39:09.846985  4128 net.cpp:131] Top shape: 20 3 32 32 (61440)
I0315 05:39:09.846988  4128 net.cpp:131] Top shape: 20 (20)
I0315 05:39:09.846990  4128 net.cpp:139] Memory required for data: 1229200
I0315 05:39:09.846994  4128 layer_factory.hpp:77] Creating layer concat_label
I0315 05:39:09.847005  4128 net.cpp:86] Creating Layer concat_label
I0315 05:39:09.847010  4128 net.cpp:408] concat_label <- label_fixed
I0315 05:39:09.847020  4128 net.cpp:408] concat_label <- label_random
I0315 05:39:09.847035  4128 net.cpp:382] concat_label -> label
I0315 05:39:09.847064  4128 net.cpp:124] Setting up concat_label
I0315 05:39:09.847067  4128 net.cpp:131] Top shape: 100 (100)
I0315 05:39:09.847069  4128 net.cpp:139] Memory required for data: 1229600
I0315 05:39:09.847071  4128 layer_factory.hpp:77] Creating layer concat_data
I0315 05:39:09.847076  4128 net.cpp:86] Creating Layer concat_data
I0315 05:39:09.847079  4128 net.cpp:408] concat_data <- data_fixed
I0315 05:39:09.847081  4128 net.cpp:408] concat_data <- data_random
I0315 05:39:09.847084  4128 net.cpp:382] concat_data -> data
I0315 05:39:09.847101  4128 net.cpp:124] Setting up concat_data
I0315 05:39:09.847105  4128 net.cpp:131] Top shape: 100 3 32 32 (307200)
I0315 05:39:09.847107  4128 net.cpp:139] Memory required for data: 2458400
I0315 05:39:09.847110  4128 layer_factory.hpp:77] Creating layer conv1
I0315 05:39:09.847124  4128 net.cpp:86] Creating Layer conv1
I0315 05:39:09.847127  4128 net.cpp:408] conv1 <- data
I0315 05:39:09.847131  4128 net.cpp:382] conv1 -> conv1
I0315 05:39:10.425870  4128 net.cpp:124] Setting up conv1
I0315 05:39:10.425906  4128 net.cpp:131] Top shape: 100 32 32 32 (3276800)
I0315 05:39:10.425911  4128 net.cpp:139] Memory required for data: 15565600
I0315 05:39:10.425940  4128 layer_factory.hpp:77] Creating layer pool1
I0315 05:39:10.425956  4128 net.cpp:86] Creating Layer pool1
I0315 05:39:10.425959  4128 net.cpp:408] pool1 <- conv1
I0315 05:39:10.425967  4128 net.cpp:382] pool1 -> pool1
I0315 05:39:10.426039  4128 net.cpp:124] Setting up pool1
I0315 05:39:10.426049  4128 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:39:10.426054  4128 net.cpp:139] Memory required for data: 18842400
I0315 05:39:10.426057  4128 layer_factory.hpp:77] Creating layer relu1
I0315 05:39:10.426064  4128 net.cpp:86] Creating Layer relu1
I0315 05:39:10.426069  4128 net.cpp:408] relu1 <- pool1
I0315 05:39:10.426096  4128 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:39:10.426323  4128 net.cpp:124] Setting up relu1
I0315 05:39:10.426334  4128 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:39:10.426340  4128 net.cpp:139] Memory required for data: 22119200
I0315 05:39:10.426344  4128 layer_factory.hpp:77] Creating layer norm1
I0315 05:39:10.426357  4128 net.cpp:86] Creating Layer norm1
I0315 05:39:10.426362  4128 net.cpp:408] norm1 <- pool1
I0315 05:39:10.426370  4128 net.cpp:382] norm1 -> norm1
I0315 05:39:10.428521  4128 net.cpp:124] Setting up norm1
I0315 05:39:10.428550  4128 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:39:10.428552  4128 net.cpp:139] Memory required for data: 25396000
I0315 05:39:10.428558  4128 layer_factory.hpp:77] Creating layer conv2
I0315 05:39:10.428575  4128 net.cpp:86] Creating Layer conv2
I0315 05:39:10.428578  4128 net.cpp:408] conv2 <- norm1
I0315 05:39:10.428586  4128 net.cpp:382] conv2 -> conv2
I0315 05:39:10.429935  4128 net.cpp:124] Setting up conv2
I0315 05:39:10.429955  4128 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:39:10.429957  4128 net.cpp:139] Memory required for data: 28672800
I0315 05:39:10.429970  4128 layer_factory.hpp:77] Creating layer relu2
I0315 05:39:10.429978  4128 net.cpp:86] Creating Layer relu2
I0315 05:39:10.429982  4128 net.cpp:408] relu2 <- conv2
I0315 05:39:10.429987  4128 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:39:10.430619  4128 net.cpp:124] Setting up relu2
I0315 05:39:10.430632  4128 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:39:10.430634  4128 net.cpp:139] Memory required for data: 31949600
I0315 05:39:10.430637  4128 layer_factory.hpp:77] Creating layer pool2
I0315 05:39:10.430644  4128 net.cpp:86] Creating Layer pool2
I0315 05:39:10.430647  4128 net.cpp:408] pool2 <- conv2
I0315 05:39:10.430654  4128 net.cpp:382] pool2 -> pool2
I0315 05:39:10.430793  4128 net.cpp:124] Setting up pool2
I0315 05:39:10.430799  4128 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:39:10.430801  4128 net.cpp:139] Memory required for data: 32768800
I0315 05:39:10.430804  4128 layer_factory.hpp:77] Creating layer norm2
I0315 05:39:10.430809  4128 net.cpp:86] Creating Layer norm2
I0315 05:39:10.430812  4128 net.cpp:408] norm2 <- pool2
I0315 05:39:10.430816  4128 net.cpp:382] norm2 -> norm2
I0315 05:39:10.431001  4128 net.cpp:124] Setting up norm2
I0315 05:39:10.431007  4128 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:39:10.431010  4128 net.cpp:139] Memory required for data: 33588000
I0315 05:39:10.431011  4128 layer_factory.hpp:77] Creating layer conv3
I0315 05:39:10.431021  4128 net.cpp:86] Creating Layer conv3
I0315 05:39:10.431025  4128 net.cpp:408] conv3 <- norm2
I0315 05:39:10.431027  4128 net.cpp:382] conv3 -> conv3
I0315 05:39:10.432492  4128 net.cpp:124] Setting up conv3
I0315 05:39:10.432507  4128 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:39:10.432509  4128 net.cpp:139] Memory required for data: 35226400
I0315 05:39:10.432519  4128 layer_factory.hpp:77] Creating layer relu3
I0315 05:39:10.432526  4128 net.cpp:86] Creating Layer relu3
I0315 05:39:10.432528  4128 net.cpp:408] relu3 <- conv3
I0315 05:39:10.432533  4128 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:39:10.432646  4128 net.cpp:124] Setting up relu3
I0315 05:39:10.432651  4128 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:39:10.432653  4128 net.cpp:139] Memory required for data: 36864800
I0315 05:39:10.432656  4128 layer_factory.hpp:77] Creating layer pool3
I0315 05:39:10.432659  4128 net.cpp:86] Creating Layer pool3
I0315 05:39:10.432662  4128 net.cpp:408] pool3 <- conv3
I0315 05:39:10.432665  4128 net.cpp:382] pool3 -> pool3
I0315 05:39:10.433286  4128 net.cpp:124] Setting up pool3
I0315 05:39:10.433296  4128 net.cpp:131] Top shape: 100 64 4 4 (102400)
I0315 05:39:10.433298  4128 net.cpp:139] Memory required for data: 37274400
I0315 05:39:10.433300  4128 layer_factory.hpp:77] Creating layer ip1
I0315 05:39:10.433306  4128 net.cpp:86] Creating Layer ip1
I0315 05:39:10.433308  4128 net.cpp:408] ip1 <- pool3
I0315 05:39:10.433331  4128 net.cpp:382] ip1 -> ip1
I0315 05:39:10.434244  4128 net.cpp:124] Setting up ip1
I0315 05:39:10.434254  4128 net.cpp:131] Top shape: 100 10 (1000)
I0315 05:39:10.434257  4128 net.cpp:139] Memory required for data: 37278400
I0315 05:39:10.434262  4128 layer_factory.hpp:77] Creating layer loss
I0315 05:39:10.434269  4128 net.cpp:86] Creating Layer loss
I0315 05:39:10.434273  4128 net.cpp:408] loss <- ip1
I0315 05:39:10.434275  4128 net.cpp:408] loss <- label
I0315 05:39:10.434280  4128 net.cpp:382] loss -> loss
I0315 05:39:10.434290  4128 layer_factory.hpp:77] Creating layer loss
I0315 05:39:10.434507  4128 net.cpp:124] Setting up loss
I0315 05:39:10.434515  4128 net.cpp:131] Top shape: (1)
I0315 05:39:10.434517  4128 net.cpp:134]     with loss weight 1
I0315 05:39:10.434535  4128 net.cpp:139] Memory required for data: 37278404
I0315 05:39:10.434536  4128 net.cpp:200] loss needs backward computation.
I0315 05:39:10.434541  4128 net.cpp:200] ip1 needs backward computation.
I0315 05:39:10.434545  4128 net.cpp:200] pool3 needs backward computation.
I0315 05:39:10.434546  4128 net.cpp:200] relu3 needs backward computation.
I0315 05:39:10.434548  4128 net.cpp:200] conv3 needs backward computation.
I0315 05:39:10.434551  4128 net.cpp:200] norm2 needs backward computation.
I0315 05:39:10.434553  4128 net.cpp:200] pool2 needs backward computation.
I0315 05:39:10.434556  4128 net.cpp:200] relu2 needs backward computation.
I0315 05:39:10.434557  4128 net.cpp:200] conv2 needs backward computation.
I0315 05:39:10.434559  4128 net.cpp:200] norm1 needs backward computation.
I0315 05:39:10.434562  4128 net.cpp:200] relu1 needs backward computation.
I0315 05:39:10.434564  4128 net.cpp:200] pool1 needs backward computation.
I0315 05:39:10.434566  4128 net.cpp:200] conv1 needs backward computation.
I0315 05:39:10.434569  4128 net.cpp:202] concat_data does not need backward computation.
I0315 05:39:10.434572  4128 net.cpp:202] concat_label does not need backward computation.
I0315 05:39:10.434576  4128 net.cpp:202] cifar2 does not need backward computation.
I0315 05:39:10.434577  4128 net.cpp:202] cifar1 does not need backward computation.
I0315 05:39:10.434579  4128 net.cpp:244] This network produces output loss
I0315 05:39:10.434593  4128 net.cpp:257] Network initialization done.
I0315 05:39:10.434787  4128 solver.cpp:173] Creating test net (#0) specified by net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:39:10.434810  4128 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar1
I0315 05:39:10.434813  4128 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar2
I0315 05:39:10.434815  4128 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_label
I0315 05:39:10.434818  4128 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_data
I0315 05:39:10.434902  4128 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 10000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Python"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
  python_param {
    module: "cifar10_base"
    layer: "Accuracy"
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:39:10.434969  4128 layer_factory.hpp:77] Creating layer cifar
I0315 05:39:10.435019  4128 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0315 05:39:10.435039  4128 net.cpp:86] Creating Layer cifar
I0315 05:39:10.435044  4128 net.cpp:382] cifar -> data
I0315 05:39:10.435050  4128 net.cpp:382] cifar -> label
I0315 05:39:10.435056  4128 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:39:10.435175  4128 data_layer.cpp:45] output data size: 10000,3,32,32
I0315 05:39:10.684134  4128 net.cpp:124] Setting up cifar
I0315 05:39:10.684156  4128 net.cpp:131] Top shape: 10000 3 32 32 (30720000)
I0315 05:39:10.684161  4128 net.cpp:131] Top shape: 10000 (10000)
I0315 05:39:10.684164  4128 net.cpp:139] Memory required for data: 122920000
I0315 05:39:10.684168  4128 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0315 05:39:10.684180  4128 net.cpp:86] Creating Layer label_cifar_1_split
I0315 05:39:10.684182  4128 net.cpp:408] label_cifar_1_split <- label
I0315 05:39:10.684188  4128 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_0
I0315 05:39:10.684196  4128 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_1
I0315 05:39:10.684288  4128 net.cpp:124] Setting up label_cifar_1_split
I0315 05:39:10.684293  4128 net.cpp:131] Top shape: 10000 (10000)
I0315 05:39:10.684294  4128 net.cpp:131] Top shape: 10000 (10000)
I0315 05:39:10.684296  4128 net.cpp:139] Memory required for data: 123000000
I0315 05:39:10.684298  4128 layer_factory.hpp:77] Creating layer conv1
I0315 05:39:10.684310  4128 net.cpp:86] Creating Layer conv1
I0315 05:39:10.684314  4128 net.cpp:408] conv1 <- data
I0315 05:39:10.684316  4128 net.cpp:382] conv1 -> conv1
I0315 05:39:10.709990  4128 net.cpp:124] Setting up conv1
I0315 05:39:10.710017  4128 net.cpp:131] Top shape: 10000 32 32 32 (327680000)
I0315 05:39:10.710019  4128 net.cpp:139] Memory required for data: 1433720000
I0315 05:39:10.710034  4128 layer_factory.hpp:77] Creating layer pool1
I0315 05:39:10.710044  4128 net.cpp:86] Creating Layer pool1
I0315 05:39:10.710047  4128 net.cpp:408] pool1 <- conv1
I0315 05:39:10.710052  4128 net.cpp:382] pool1 -> pool1
I0315 05:39:10.710108  4128 net.cpp:124] Setting up pool1
I0315 05:39:10.710113  4128 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:39:10.710115  4128 net.cpp:139] Memory required for data: 1761400000
I0315 05:39:10.710117  4128 layer_factory.hpp:77] Creating layer relu1
I0315 05:39:10.710122  4128 net.cpp:86] Creating Layer relu1
I0315 05:39:10.710125  4128 net.cpp:408] relu1 <- pool1
I0315 05:39:10.710129  4128 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:39:10.710942  4128 net.cpp:124] Setting up relu1
I0315 05:39:10.710958  4128 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:39:10.710960  4128 net.cpp:139] Memory required for data: 2089080000
I0315 05:39:10.710963  4128 layer_factory.hpp:77] Creating layer norm1
I0315 05:39:10.710973  4128 net.cpp:86] Creating Layer norm1
I0315 05:39:10.710976  4128 net.cpp:408] norm1 <- pool1
I0315 05:39:10.710981  4128 net.cpp:382] norm1 -> norm1
I0315 05:39:10.712628  4128 net.cpp:124] Setting up norm1
I0315 05:39:10.712651  4128 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:39:10.712652  4128 net.cpp:139] Memory required for data: 2416760000
I0315 05:39:10.712657  4128 layer_factory.hpp:77] Creating layer conv2
I0315 05:39:10.712671  4128 net.cpp:86] Creating Layer conv2
I0315 05:39:10.712674  4128 net.cpp:408] conv2 <- norm1
I0315 05:39:10.712682  4128 net.cpp:382] conv2 -> conv2
I0315 05:39:10.715602  4128 net.cpp:124] Setting up conv2
I0315 05:39:10.715628  4128 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:39:10.715631  4128 net.cpp:139] Memory required for data: 2744440000
I0315 05:39:10.715646  4128 layer_factory.hpp:77] Creating layer relu2
I0315 05:39:10.715654  4128 net.cpp:86] Creating Layer relu2
I0315 05:39:10.715658  4128 net.cpp:408] relu2 <- conv2
I0315 05:39:10.715662  4128 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:39:10.716464  4128 net.cpp:124] Setting up relu2
I0315 05:39:10.716483  4128 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:39:10.716485  4128 net.cpp:139] Memory required for data: 3072120000
I0315 05:39:10.716490  4128 layer_factory.hpp:77] Creating layer pool2
I0315 05:39:10.716500  4128 net.cpp:86] Creating Layer pool2
I0315 05:39:10.716503  4128 net.cpp:408] pool2 <- conv2
I0315 05:39:10.716511  4128 net.cpp:382] pool2 -> pool2
I0315 05:39:10.716658  4128 net.cpp:124] Setting up pool2
I0315 05:39:10.716667  4128 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:39:10.716670  4128 net.cpp:139] Memory required for data: 3154040000
I0315 05:39:10.716672  4128 layer_factory.hpp:77] Creating layer norm2
I0315 05:39:10.716678  4128 net.cpp:86] Creating Layer norm2
I0315 05:39:10.716681  4128 net.cpp:408] norm2 <- pool2
I0315 05:39:10.716686  4128 net.cpp:382] norm2 -> norm2
I0315 05:39:10.717861  4128 net.cpp:124] Setting up norm2
I0315 05:39:10.717875  4128 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:39:10.717878  4128 net.cpp:139] Memory required for data: 3235960000
I0315 05:39:10.717880  4128 layer_factory.hpp:77] Creating layer conv3
I0315 05:39:10.717891  4128 net.cpp:86] Creating Layer conv3
I0315 05:39:10.717893  4128 net.cpp:408] conv3 <- norm2
I0315 05:39:10.717898  4128 net.cpp:382] conv3 -> conv3
I0315 05:39:10.719517  4128 net.cpp:124] Setting up conv3
I0315 05:39:10.719538  4128 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:39:10.719540  4128 net.cpp:139] Memory required for data: 3399800000
I0315 05:39:10.719550  4128 layer_factory.hpp:77] Creating layer relu3
I0315 05:39:10.719558  4128 net.cpp:86] Creating Layer relu3
I0315 05:39:10.719561  4128 net.cpp:408] relu3 <- conv3
I0315 05:39:10.719566  4128 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:39:10.719687  4128 net.cpp:124] Setting up relu3
I0315 05:39:10.719693  4128 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:39:10.719696  4128 net.cpp:139] Memory required for data: 3563640000
I0315 05:39:10.719698  4128 layer_factory.hpp:77] Creating layer pool3
I0315 05:39:10.719704  4128 net.cpp:86] Creating Layer pool3
I0315 05:39:10.719707  4128 net.cpp:408] pool3 <- conv3
I0315 05:39:10.719725  4128 net.cpp:382] pool3 -> pool3
I0315 05:39:10.720365  4128 net.cpp:124] Setting up pool3
I0315 05:39:10.720374  4128 net.cpp:131] Top shape: 10000 64 4 4 (10240000)
I0315 05:39:10.720376  4128 net.cpp:139] Memory required for data: 3604600000
I0315 05:39:10.720379  4128 layer_factory.hpp:77] Creating layer ip1
I0315 05:39:10.720386  4128 net.cpp:86] Creating Layer ip1
I0315 05:39:10.720388  4128 net.cpp:408] ip1 <- pool3
I0315 05:39:10.720392  4128 net.cpp:382] ip1 -> ip1
I0315 05:39:10.720571  4128 net.cpp:124] Setting up ip1
I0315 05:39:10.720576  4128 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:39:10.720577  4128 net.cpp:139] Memory required for data: 3605000000
I0315 05:39:10.720582  4128 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0315 05:39:10.720588  4128 net.cpp:86] Creating Layer ip1_ip1_0_split
I0315 05:39:10.720592  4128 net.cpp:408] ip1_ip1_0_split <- ip1
I0315 05:39:10.720594  4128 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0315 05:39:10.720599  4128 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0315 05:39:10.720624  4128 net.cpp:124] Setting up ip1_ip1_0_split
I0315 05:39:10.720628  4128 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:39:10.720630  4128 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:39:10.720633  4128 net.cpp:139] Memory required for data: 3605800000
I0315 05:39:10.720634  4128 layer_factory.hpp:77] Creating layer accuracy
I0315 05:39:10.780580  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:10.890380  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:11.003350  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:11.108157  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:11.565763  4128 net.cpp:86] Creating Layer accuracy
I0315 05:39:11.565785  4128 net.cpp:408] accuracy <- ip1_ip1_0_split_0
I0315 05:39:11.565793  4128 net.cpp:408] accuracy <- label_cifar_1_split_0
I0315 05:39:11.565798  4128 net.cpp:382] accuracy -> accuracy
I0315 05:39:11.566823  4128 net.cpp:124] Setting up accuracy
I0315 05:39:11.566839  4128 net.cpp:131] Top shape: 1 (1)
I0315 05:39:11.566841  4128 net.cpp:139] Memory required for data: 3605800004
I0315 05:39:11.566845  4128 layer_factory.hpp:77] Creating layer loss
I0315 05:39:11.566855  4128 net.cpp:86] Creating Layer loss
I0315 05:39:11.566859  4128 net.cpp:408] loss <- ip1_ip1_0_split_1
I0315 05:39:11.566865  4128 net.cpp:408] loss <- label_cifar_1_split_1
I0315 05:39:11.566871  4128 net.cpp:382] loss -> loss
I0315 05:39:11.566889  4128 layer_factory.hpp:77] Creating layer loss
I0315 05:39:11.567227  4128 net.cpp:124] Setting up loss
I0315 05:39:11.567235  4128 net.cpp:131] Top shape: (1)
I0315 05:39:11.567237  4128 net.cpp:134]     with loss weight 1
I0315 05:39:11.567246  4128 net.cpp:139] Memory required for data: 3605800008
I0315 05:39:11.567250  4128 net.cpp:200] loss needs backward computation.
I0315 05:39:11.567252  4128 net.cpp:202] accuracy does not need backward computation.
I0315 05:39:11.567255  4128 net.cpp:200] ip1_ip1_0_split needs backward computation.
I0315 05:39:11.567257  4128 net.cpp:200] ip1 needs backward computation.
I0315 05:39:11.567260  4128 net.cpp:200] pool3 needs backward computation.
I0315 05:39:11.567261  4128 net.cpp:200] relu3 needs backward computation.
I0315 05:39:11.567263  4128 net.cpp:200] conv3 needs backward computation.
I0315 05:39:11.567265  4128 net.cpp:200] norm2 needs backward computation.
I0315 05:39:11.567267  4128 net.cpp:200] pool2 needs backward computation.
I0315 05:39:11.567270  4128 net.cpp:200] relu2 needs backward computation.
I0315 05:39:11.567272  4128 net.cpp:200] conv2 needs backward computation.
I0315 05:39:11.567275  4128 net.cpp:200] norm1 needs backward computation.
I0315 05:39:11.567278  4128 net.cpp:200] relu1 needs backward computation.
I0315 05:39:11.567281  4128 net.cpp:200] pool1 needs backward computation.
I0315 05:39:11.567282  4128 net.cpp:200] conv1 needs backward computation.
I0315 05:39:11.567284  4128 net.cpp:202] label_cifar_1_split does not need backward computation.
I0315 05:39:11.567304  4128 net.cpp:202] cifar does not need backward computation.
I0315 05:39:11.567306  4128 net.cpp:244] This network produces output accuracy
I0315 05:39:11.567309  4128 net.cpp:244] This network produces output loss
I0315 05:39:11.567319  4128 net.cpp:257] Network initialization done.
I0315 05:39:11.567378  4128 solver.cpp:56] Solver scaffolding done.
I0315 05:39:11.567610  4128 caffe.cpp:242] Resuming from examples/cifar10/cifar10_full/_iter_30000.solverstate.h5
I0315 05:39:11.569205  4128 net.cpp:801] Ignoring source layer cifar
I0315 05:39:11.569278  4128 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0315 05:39:11.571285  4128 caffe.cpp:248] Starting Optimization
I0315 05:39:11.571300  4128 solver.cpp:273] Solving CIFAR10_full
I0315 05:39:11.571302  4128 solver.cpp:274] Learning Rate Policy: fixed
I0315 05:39:11.571305  4128 solver.cpp:275] resume_file
I0315 05:39:11.571478  4128 solver.cpp:332] Iteration 30000, Testing net (#0)
I0315 05:39:11.571485  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:39:11.571490  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:39:11.571491  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:39:11.571492  4128 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:39:12.188576  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7334
I0315 05:39:12.188601  4128 solver.cpp:399]     Test net output #1: loss = 0.835263 (* 1 = 0.835263 loss)
I0315 05:39:12.199426  4128 solver.cpp:219] Iteration 30000 (-21026.2 iter/s, 0.628062s/200 iters), loss = 0.438773
I0315 05:39:12.199471  4128 solver.cpp:238]     Train net output #0: loss = 0.438773 (* 1 = 0.438773 loss)
I0315 05:39:12.199478  4128 sgd_solver.cpp:105] Iteration 30000, lr = 0.0005
I0315 05:39:14.412199  4128 solver.cpp:219] Iteration 30200 (90.3902 iter/s, 2.21263s/200 iters), loss = 0.37999
I0315 05:39:14.412247  4128 solver.cpp:238]     Train net output #0: loss = 0.37999 (* 1 = 0.37999 loss)
I0315 05:39:14.412252  4128 sgd_solver.cpp:105] Iteration 30200, lr = 0.0005
I0315 05:39:14.862692  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:16.560914  4128 solver.cpp:219] Iteration 30400 (93.0852 iter/s, 2.14857s/200 iters), loss = 0.444645
I0315 05:39:16.560967  4128 solver.cpp:238]     Train net output #0: loss = 0.444645 (* 1 = 0.444645 loss)
I0315 05:39:16.560973  4128 sgd_solver.cpp:105] Iteration 30400, lr = 0.0005
I0315 05:39:17.614516  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:18.768165  4128 solver.cpp:219] Iteration 30600 (90.6165 iter/s, 2.2071s/200 iters), loss = 0.544446
I0315 05:39:18.768210  4128 solver.cpp:238]     Train net output #0: loss = 0.544446 (* 1 = 0.544446 loss)
I0315 05:39:18.768216  4128 sgd_solver.cpp:105] Iteration 30600, lr = 0.0005
I0315 05:39:20.367655  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:20.964331  4128 solver.cpp:219] Iteration 30800 (91.0737 iter/s, 2.19602s/200 iters), loss = 0.329434
I0315 05:39:20.964380  4128 solver.cpp:238]     Train net output #0: loss = 0.329434 (* 1 = 0.329434 loss)
I0315 05:39:20.964385  4128 sgd_solver.cpp:105] Iteration 30800, lr = 0.0005
I0315 05:39:23.119215  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:23.119859  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:23.156309  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_31000.caffemodel.h5
I0315 05:39:23.162333  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_31000.solverstate.h5
I0315 05:39:23.163094  4128 solver.cpp:332] Iteration 31000, Testing net (#0)
I0315 05:39:23.163105  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:39:23.163106  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:39:23.163108  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:39:23.163110  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:39:23.250159  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:39:23.793023  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7465
I0315 05:39:23.793048  4128 solver.cpp:399]     Test net output #1: loss = 0.748055 (* 1 = 0.748055 loss)
I0315 05:39:23.803032  4128 solver.cpp:219] Iteration 31000 (70.459 iter/s, 2.83853s/200 iters), loss = 0.426841
I0315 05:39:23.803082  4128 solver.cpp:238]     Train net output #0: loss = 0.426841 (* 1 = 0.426841 loss)
I0315 05:39:23.803091  4128 sgd_solver.cpp:105] Iteration 31000, lr = 0.0005
I0315 05:39:26.001459  4128 solver.cpp:219] Iteration 31200 (90.9801 iter/s, 2.19828s/200 iters), loss = 0.35285
I0315 05:39:26.001507  4128 solver.cpp:238]     Train net output #0: loss = 0.35285 (* 1 = 0.35285 loss)
I0315 05:39:26.001513  4128 sgd_solver.cpp:105] Iteration 31200, lr = 0.0005
I0315 05:39:26.451776  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:28.155616  4128 solver.cpp:219] Iteration 31400 (92.8498 iter/s, 2.15402s/200 iters), loss = 0.445056
I0315 05:39:28.155664  4128 solver.cpp:238]     Train net output #0: loss = 0.445056 (* 1 = 0.445056 loss)
I0315 05:39:28.155670  4128 sgd_solver.cpp:105] Iteration 31400, lr = 0.0005
I0315 05:39:29.204262  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:30.363440  4128 solver.cpp:219] Iteration 31600 (90.5932 iter/s, 2.20767s/200 iters), loss = 0.530393
I0315 05:39:30.363550  4128 solver.cpp:238]     Train net output #0: loss = 0.530393 (* 1 = 0.530393 loss)
I0315 05:39:30.363576  4128 sgd_solver.cpp:105] Iteration 31600, lr = 0.0005
I0315 05:39:31.996841  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:32.597736  4128 solver.cpp:219] Iteration 31800 (89.5217 iter/s, 2.2341s/200 iters), loss = 0.3187
I0315 05:39:32.597776  4128 solver.cpp:238]     Train net output #0: loss = 0.3187 (* 1 = 0.3187 loss)
I0315 05:39:32.597782  4128 sgd_solver.cpp:105] Iteration 31800, lr = 0.0005
I0315 05:39:33.870858  4128 blocking_queue.cpp:49] Waiting for data
I0315 05:39:34.754108  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:34.754511  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:34.787425  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_32000.caffemodel.h5
I0315 05:39:34.793503  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_32000.solverstate.h5
I0315 05:39:34.794246  4128 solver.cpp:332] Iteration 32000, Testing net (#0)
I0315 05:39:34.794255  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:39:34.794258  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:39:34.794260  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:39:34.794261  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:39:34.932237  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:39:35.419282  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7475
I0315 05:39:35.419309  4128 solver.cpp:399]     Test net output #1: loss = 0.742085 (* 1 = 0.742085 loss)
I0315 05:39:35.429188  4128 solver.cpp:219] Iteration 32000 (70.6391 iter/s, 2.83129s/200 iters), loss = 0.414837
I0315 05:39:35.429237  4128 solver.cpp:238]     Train net output #0: loss = 0.414837 (* 1 = 0.414837 loss)
I0315 05:39:35.429244  4128 sgd_solver.cpp:105] Iteration 32000, lr = 0.0005
I0315 05:39:37.648720  4128 solver.cpp:219] Iteration 32200 (90.1152 iter/s, 2.21938s/200 iters), loss = 0.343224
I0315 05:39:37.648787  4128 solver.cpp:238]     Train net output #0: loss = 0.343224 (* 1 = 0.343224 loss)
I0315 05:39:37.648795  4128 sgd_solver.cpp:105] Iteration 32200, lr = 0.0005
I0315 05:39:38.094990  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:39.800508  4128 solver.cpp:219] Iteration 32400 (92.9526 iter/s, 2.15163s/200 iters), loss = 0.438397
I0315 05:39:39.800689  4128 solver.cpp:238]     Train net output #0: loss = 0.438397 (* 1 = 0.438397 loss)
I0315 05:39:39.800698  4128 sgd_solver.cpp:105] Iteration 32400, lr = 0.0005
I0315 05:39:40.861033  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:42.022518  4128 solver.cpp:219] Iteration 32600 (90.0197 iter/s, 2.22174s/200 iters), loss = 0.518299
I0315 05:39:42.022572  4128 solver.cpp:238]     Train net output #0: loss = 0.518299 (* 1 = 0.518299 loss)
I0315 05:39:42.022579  4128 sgd_solver.cpp:105] Iteration 32600, lr = 0.0005
I0315 05:39:43.641145  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:44.240085  4128 solver.cpp:219] Iteration 32800 (90.195 iter/s, 2.21742s/200 iters), loss = 0.309366
I0315 05:39:44.240134  4128 solver.cpp:238]     Train net output #0: loss = 0.309366 (* 1 = 0.309366 loss)
I0315 05:39:44.240139  4128 sgd_solver.cpp:105] Iteration 32800, lr = 0.0005
I0315 05:39:46.337358  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:46.337494  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:46.379835  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_33000.caffemodel.h5
I0315 05:39:46.381444  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_33000.solverstate.h5
I0315 05:39:46.382562  4128 solver.cpp:332] Iteration 33000, Testing net (#0)
I0315 05:39:46.382598  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:39:46.382612  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:39:46.382624  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:39:46.382637  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:39:46.485776  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:39:47.013451  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7496
I0315 05:39:47.013478  4128 solver.cpp:399]     Test net output #1: loss = 0.737361 (* 1 = 0.737361 loss)
I0315 05:39:47.023125  4128 solver.cpp:219] Iteration 33000 (71.8683 iter/s, 2.78287s/200 iters), loss = 0.405213
I0315 05:39:47.023185  4128 solver.cpp:238]     Train net output #0: loss = 0.405213 (* 1 = 0.405213 loss)
I0315 05:39:47.023192  4128 sgd_solver.cpp:105] Iteration 33000, lr = 0.0005
I0315 05:39:49.243021  4128 solver.cpp:219] Iteration 33200 (90.1006 iter/s, 2.21974s/200 iters), loss = 0.334497
I0315 05:39:49.243064  4128 solver.cpp:238]     Train net output #0: loss = 0.334497 (* 1 = 0.334497 loss)
I0315 05:39:49.243070  4128 sgd_solver.cpp:105] Iteration 33200, lr = 0.0005
I0315 05:39:49.754778  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:51.476665  4128 solver.cpp:219] Iteration 33400 (89.5452 iter/s, 2.23351s/200 iters), loss = 0.430973
I0315 05:39:51.476703  4128 solver.cpp:238]     Train net output #0: loss = 0.430973 (* 1 = 0.430973 loss)
I0315 05:39:51.476708  4128 sgd_solver.cpp:105] Iteration 33400, lr = 0.0005
I0315 05:39:52.535274  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:53.693413  4128 solver.cpp:219] Iteration 33600 (90.2279 iter/s, 2.21661s/200 iters), loss = 0.507875
I0315 05:39:53.693460  4128 solver.cpp:238]     Train net output #0: loss = 0.507875 (* 1 = 0.507875 loss)
I0315 05:39:53.693465  4128 sgd_solver.cpp:105] Iteration 33600, lr = 0.0005
I0315 05:39:55.285513  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:55.875720  4128 solver.cpp:219] Iteration 33800 (91.6523 iter/s, 2.18216s/200 iters), loss = 0.303728
I0315 05:39:55.875777  4128 solver.cpp:238]     Train net output #0: loss = 0.303728 (* 1 = 0.303728 loss)
I0315 05:39:55.875783  4128 sgd_solver.cpp:105] Iteration 33800, lr = 0.0005
I0315 05:39:58.030963  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:58.031702  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:39:58.064064  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_34000.caffemodel.h5
I0315 05:39:58.070099  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_34000.solverstate.h5
I0315 05:39:58.070919  4128 solver.cpp:332] Iteration 34000, Testing net (#0)
I0315 05:39:58.070932  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:39:58.070935  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:39:58.070936  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:39:58.070937  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:39:58.185354  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:39:58.700367  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7505
I0315 05:39:58.700392  4128 solver.cpp:399]     Test net output #1: loss = 0.733413 (* 1 = 0.733413 loss)
I0315 05:39:58.710480  4128 solver.cpp:219] Iteration 34000 (70.5571 iter/s, 2.83458s/200 iters), loss = 0.400169
I0315 05:39:58.710522  4128 solver.cpp:238]     Train net output #0: loss = 0.400169 (* 1 = 0.400169 loss)
I0315 05:39:58.710528  4128 sgd_solver.cpp:105] Iteration 34000, lr = 0.0005
I0315 05:40:00.933203  4128 solver.cpp:219] Iteration 34200 (89.9853 iter/s, 2.22258s/200 iters), loss = 0.329938
I0315 05:40:00.933256  4128 solver.cpp:238]     Train net output #0: loss = 0.329938 (* 1 = 0.329938 loss)
I0315 05:40:00.933264  4128 sgd_solver.cpp:105] Iteration 34200, lr = 0.0005
I0315 05:40:01.438992  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:03.086100  4128 solver.cpp:219] Iteration 34400 (92.9047 iter/s, 2.15274s/200 iters), loss = 0.425012
I0315 05:40:03.086169  4128 solver.cpp:238]     Train net output #0: loss = 0.425012 (* 1 = 0.425012 loss)
I0315 05:40:03.086179  4128 sgd_solver.cpp:105] Iteration 34400, lr = 0.0005
I0315 05:40:04.146939  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:05.307958  4128 solver.cpp:219] Iteration 34600 (90.0213 iter/s, 2.2217s/200 iters), loss = 0.500073
I0315 05:40:05.308007  4128 solver.cpp:238]     Train net output #0: loss = 0.500073 (* 1 = 0.500073 loss)
I0315 05:40:05.308012  4128 sgd_solver.cpp:105] Iteration 34600, lr = 0.0005
I0315 05:40:06.928529  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:07.533789  4128 solver.cpp:219] Iteration 34800 (89.86 iter/s, 2.22568s/200 iters), loss = 0.299652
I0315 05:40:07.533902  4128 solver.cpp:238]     Train net output #0: loss = 0.299652 (* 1 = 0.299652 loss)
I0315 05:40:07.533920  4128 sgd_solver.cpp:105] Iteration 34800, lr = 0.0005
I0315 05:40:09.681221  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:09.683012  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:09.743870  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_35000.caffemodel.h5
I0315 05:40:09.749897  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_35000.solverstate.h5
I0315 05:40:09.750689  4128 solver.cpp:332] Iteration 35000, Testing net (#0)
I0315 05:40:09.750700  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:40:09.750702  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:40:09.750704  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:40:09.750705  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:40:09.880971  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:40:10.395733  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7516
I0315 05:40:10.395759  4128 solver.cpp:399]     Test net output #1: loss = 0.731469 (* 1 = 0.731469 loss)
I0315 05:40:10.405731  4128 solver.cpp:219] Iteration 35000 (69.6449 iter/s, 2.87171s/200 iters), loss = 0.396674
I0315 05:40:10.405777  4128 solver.cpp:238]     Train net output #0: loss = 0.396674 (* 1 = 0.396674 loss)
I0315 05:40:10.405784  4128 sgd_solver.cpp:105] Iteration 35000, lr = 0.0005
I0315 05:40:12.639012  4128 solver.cpp:219] Iteration 35200 (89.5601 iter/s, 2.23314s/200 iters), loss = 0.324753
I0315 05:40:12.639060  4128 solver.cpp:238]     Train net output #0: loss = 0.324753 (* 1 = 0.324753 loss)
I0315 05:40:12.639065  4128 sgd_solver.cpp:105] Iteration 35200, lr = 0.0005
I0315 05:40:13.147601  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:14.833845  4128 solver.cpp:219] Iteration 35400 (91.129 iter/s, 2.19469s/200 iters), loss = 0.41924
I0315 05:40:14.833894  4128 solver.cpp:238]     Train net output #0: loss = 0.41924 (* 1 = 0.41924 loss)
I0315 05:40:14.833899  4128 sgd_solver.cpp:105] Iteration 35400, lr = 0.0005
I0315 05:40:15.870754  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:17.029858  4128 solver.cpp:219] Iteration 35600 (91.1678 iter/s, 2.19376s/200 iters), loss = 0.493936
I0315 05:40:17.029909  4128 solver.cpp:238]     Train net output #0: loss = 0.493936 (* 1 = 0.493936 loss)
I0315 05:40:17.029917  4128 sgd_solver.cpp:105] Iteration 35600, lr = 0.0005
I0315 05:40:18.648488  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:19.252264  4128 solver.cpp:219] Iteration 35800 (89.9985 iter/s, 2.22226s/200 iters), loss = 0.294711
I0315 05:40:19.252313  4128 solver.cpp:238]     Train net output #0: loss = 0.294711 (* 1 = 0.294711 loss)
I0315 05:40:19.252320  4128 sgd_solver.cpp:105] Iteration 35800, lr = 0.0005
I0315 05:40:21.425740  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:21.426364  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:21.459074  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_36000.caffemodel.h5
I0315 05:40:21.465162  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_36000.solverstate.h5
I0315 05:40:21.465925  4128 solver.cpp:332] Iteration 36000, Testing net (#0)
I0315 05:40:21.465936  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:40:21.465939  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:40:21.465940  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:40:21.465942  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:40:21.553383  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:40:22.097430  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7526
I0315 05:40:22.097455  4128 solver.cpp:399]     Test net output #1: loss = 0.729447 (* 1 = 0.729447 loss)
I0315 05:40:22.107250  4128 solver.cpp:219] Iteration 36000 (70.0571 iter/s, 2.85482s/200 iters), loss = 0.391195
I0315 05:40:22.107295  4128 solver.cpp:238]     Train net output #0: loss = 0.391195 (* 1 = 0.391195 loss)
I0315 05:40:22.107300  4128 sgd_solver.cpp:105] Iteration 36000, lr = 0.0005
I0315 05:40:24.329062  4128 solver.cpp:219] Iteration 36200 (90.0222 iter/s, 2.22167s/200 iters), loss = 0.320876
I0315 05:40:24.329100  4128 solver.cpp:238]     Train net output #0: loss = 0.320876 (* 1 = 0.320876 loss)
I0315 05:40:24.329105  4128 sgd_solver.cpp:105] Iteration 36200, lr = 0.0005
I0315 05:40:24.792464  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:26.504390  4128 solver.cpp:219] Iteration 36400 (91.9456 iter/s, 2.1752s/200 iters), loss = 0.413438
I0315 05:40:26.504429  4128 solver.cpp:238]     Train net output #0: loss = 0.413438 (* 1 = 0.413438 loss)
I0315 05:40:26.504434  4128 sgd_solver.cpp:105] Iteration 36400, lr = 0.0005
I0315 05:40:27.567785  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:28.742743  4128 solver.cpp:219] Iteration 36600 (89.357 iter/s, 2.23821s/200 iters), loss = 0.489415
I0315 05:40:28.742787  4128 solver.cpp:238]     Train net output #0: loss = 0.489415 (* 1 = 0.489415 loss)
I0315 05:40:28.742792  4128 sgd_solver.cpp:105] Iteration 36600, lr = 0.0005
I0315 05:40:30.370926  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:30.970103  4128 solver.cpp:219] Iteration 36800 (89.798 iter/s, 2.22722s/200 iters), loss = 0.290688
I0315 05:40:30.970151  4128 solver.cpp:238]     Train net output #0: loss = 0.290688 (* 1 = 0.290688 loss)
I0315 05:40:30.970157  4128 sgd_solver.cpp:105] Iteration 36800, lr = 0.0005
I0315 05:40:33.105453  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:33.111760  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:33.132603  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_37000.caffemodel.h5
I0315 05:40:33.138919  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_37000.solverstate.h5
I0315 05:40:33.141369  4128 solver.cpp:332] Iteration 37000, Testing net (#0)
I0315 05:40:33.141417  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:40:33.141428  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:40:33.141435  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:40:33.141445  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:40:33.231777  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:40:33.790783  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7527
I0315 05:40:33.790814  4128 solver.cpp:399]     Test net output #1: loss = 0.727328 (* 1 = 0.727328 loss)
I0315 05:40:33.807865  4128 solver.cpp:219] Iteration 37000 (70.4823 iter/s, 2.83759s/200 iters), loss = 0.387533
I0315 05:40:33.807912  4128 solver.cpp:238]     Train net output #0: loss = 0.387533 (* 1 = 0.387533 loss)
I0315 05:40:33.807917  4128 sgd_solver.cpp:105] Iteration 37000, lr = 0.0005
I0315 05:40:35.967937  4128 solver.cpp:219] Iteration 37200 (92.5957 iter/s, 2.15993s/200 iters), loss = 0.316071
I0315 05:40:35.968001  4128 solver.cpp:238]     Train net output #0: loss = 0.316071 (* 1 = 0.316071 loss)
I0315 05:40:35.968008  4128 sgd_solver.cpp:105] Iteration 37200, lr = 0.0005
I0315 05:40:36.479522  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:38.206363  4128 solver.cpp:219] Iteration 37400 (89.3549 iter/s, 2.23827s/200 iters), loss = 0.408625
I0315 05:40:38.206423  4128 solver.cpp:238]     Train net output #0: loss = 0.408625 (* 1 = 0.408625 loss)
I0315 05:40:38.206431  4128 sgd_solver.cpp:105] Iteration 37400, lr = 0.0005
I0315 05:40:39.261869  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:40.433086  4128 solver.cpp:219] Iteration 37600 (89.8244 iter/s, 2.22657s/200 iters), loss = 0.485252
I0315 05:40:40.433257  4128 solver.cpp:238]     Train net output #0: loss = 0.485252 (* 1 = 0.485252 loss)
I0315 05:40:40.433265  4128 sgd_solver.cpp:105] Iteration 37600, lr = 0.0005
I0315 05:40:42.005825  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:42.596874  4128 solver.cpp:219] Iteration 37800 (92.4418 iter/s, 2.16352s/200 iters), loss = 0.288185
I0315 05:40:42.596935  4128 solver.cpp:238]     Train net output #0: loss = 0.288186 (* 1 = 0.288186 loss)
I0315 05:40:42.596941  4128 sgd_solver.cpp:105] Iteration 37800, lr = 0.0005
I0315 05:40:44.778360  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:44.778579  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:44.811828  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_38000.caffemodel.h5
I0315 05:40:44.817811  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_38000.solverstate.h5
I0315 05:40:44.818532  4128 solver.cpp:332] Iteration 38000, Testing net (#0)
I0315 05:40:44.818542  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:40:44.818544  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:40:44.818545  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:40:44.818547  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:40:44.915686  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:40:45.445848  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7536
I0315 05:40:45.445874  4128 solver.cpp:399]     Test net output #1: loss = 0.726197 (* 1 = 0.726197 loss)
I0315 05:40:45.455329  4128 solver.cpp:219] Iteration 38000 (69.9722 iter/s, 2.85828s/200 iters), loss = 0.385075
I0315 05:40:45.455374  4128 solver.cpp:238]     Train net output #0: loss = 0.385075 (* 1 = 0.385075 loss)
I0315 05:40:45.455380  4128 sgd_solver.cpp:105] Iteration 38000, lr = 0.0005
I0315 05:40:47.693472  4128 solver.cpp:219] Iteration 38200 (89.3656 iter/s, 2.238s/200 iters), loss = 0.313465
I0315 05:40:47.693533  4128 solver.cpp:238]     Train net output #0: loss = 0.313465 (* 1 = 0.313465 loss)
I0315 05:40:47.693542  4128 sgd_solver.cpp:105] Iteration 38200, lr = 0.0005
I0315 05:40:48.197765  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:49.928587  4128 solver.cpp:219] Iteration 38400 (89.4871 iter/s, 2.23496s/200 iters), loss = 0.405017
I0315 05:40:49.928637  4128 solver.cpp:238]     Train net output #0: loss = 0.405017 (* 1 = 0.405017 loss)
I0315 05:40:49.928642  4128 sgd_solver.cpp:105] Iteration 38400, lr = 0.0005
I0315 05:40:50.989835  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:52.151510  4128 solver.cpp:219] Iteration 38600 (89.9775 iter/s, 2.22278s/200 iters), loss = 0.48194
I0315 05:40:52.151556  4128 solver.cpp:238]     Train net output #0: loss = 0.48194 (* 1 = 0.48194 loss)
I0315 05:40:52.151561  4128 sgd_solver.cpp:105] Iteration 38600, lr = 0.0005
I0315 05:40:53.728579  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:54.354459  4128 solver.cpp:219] Iteration 38800 (90.7933 iter/s, 2.20281s/200 iters), loss = 0.284382
I0315 05:40:54.354502  4128 solver.cpp:238]     Train net output #0: loss = 0.284382 (* 1 = 0.284382 loss)
I0315 05:40:54.354507  4128 sgd_solver.cpp:105] Iteration 38800, lr = 0.0005
I0315 05:40:56.495352  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:56.495669  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:40:56.529330  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_39000.caffemodel.h5
I0315 05:40:56.535571  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_39000.solverstate.h5
I0315 05:40:56.536381  4128 solver.cpp:332] Iteration 39000, Testing net (#0)
I0315 05:40:56.536396  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:40:56.536398  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:40:56.536399  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:40:56.536418  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:40:56.621106  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:40:57.161455  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7543
I0315 05:40:57.161481  4128 solver.cpp:399]     Test net output #1: loss = 0.724612 (* 1 = 0.724612 loss)
I0315 05:40:57.171279  4128 solver.cpp:219] Iteration 39000 (71.061 iter/s, 2.81448s/200 iters), loss = 0.382792
I0315 05:40:57.171322  4128 solver.cpp:238]     Train net output #0: loss = 0.382792 (* 1 = 0.382792 loss)
I0315 05:40:57.171329  4128 sgd_solver.cpp:105] Iteration 39000, lr = 0.0005
I0315 05:40:59.396836  4128 solver.cpp:219] Iteration 39200 (89.8708 iter/s, 2.22542s/200 iters), loss = 0.312228
I0315 05:40:59.396880  4128 solver.cpp:238]     Train net output #0: loss = 0.312228 (* 1 = 0.312228 loss)
I0315 05:40:59.396885  4128 sgd_solver.cpp:105] Iteration 39200, lr = 0.0005
I0315 05:40:59.907204  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:01.606256  4128 solver.cpp:219] Iteration 39400 (90.5272 iter/s, 2.20928s/200 iters), loss = 0.401389
I0315 05:41:01.606305  4128 solver.cpp:238]     Train net output #0: loss = 0.401389 (* 1 = 0.401389 loss)
I0315 05:41:01.606312  4128 sgd_solver.cpp:105] Iteration 39400, lr = 0.0005
I0315 05:41:02.659559  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:03.774080  4128 solver.cpp:219] Iteration 39600 (92.2647 iter/s, 2.16768s/200 iters), loss = 0.477377
I0315 05:41:03.774139  4128 solver.cpp:238]     Train net output #0: loss = 0.477378 (* 1 = 0.477378 loss)
I0315 05:41:03.774147  4128 sgd_solver.cpp:105] Iteration 39600, lr = 0.0005
I0315 05:41:05.401021  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:06.009213  4128 solver.cpp:219] Iteration 39800 (89.4866 iter/s, 2.23497s/200 iters), loss = 0.282177
I0315 05:41:06.009281  4128 solver.cpp:238]     Train net output #0: loss = 0.282177 (* 1 = 0.282177 loss)
I0315 05:41:06.009292  4128 sgd_solver.cpp:105] Iteration 39800, lr = 0.0005
I0315 05:41:08.201642  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:08.202775  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:08.236943  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_40000.caffemodel.h5
I0315 05:41:08.242939  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_40000.solverstate.h5
I0315 05:41:08.243700  4128 solver.cpp:332] Iteration 40000, Testing net (#0)
I0315 05:41:08.243708  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:41:08.243711  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:41:08.243712  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:41:08.243715  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:41:08.391181  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:41:08.883280  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7539
I0315 05:41:08.883309  4128 solver.cpp:399]     Test net output #1: loss = 0.723892 (* 1 = 0.723892 loss)
I0315 05:41:08.893072  4128 solver.cpp:219] Iteration 40000 (69.3561 iter/s, 2.88367s/200 iters), loss = 0.380291
I0315 05:41:08.893134  4128 solver.cpp:238]     Train net output #0: loss = 0.380291 (* 1 = 0.380291 loss)
I0315 05:41:08.893143  4128 sgd_solver.cpp:105] Iteration 40000, lr = 0.0005
I0315 05:41:11.123456  4128 solver.cpp:219] Iteration 40200 (89.6771 iter/s, 2.23022s/200 iters), loss = 0.310303
I0315 05:41:11.123680  4128 solver.cpp:238]     Train net output #0: loss = 0.310304 (* 1 = 0.310304 loss)
I0315 05:41:11.123690  4128 sgd_solver.cpp:105] Iteration 40200, lr = 0.0005
I0315 05:41:11.576748  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:13.303637  4128 solver.cpp:219] Iteration 40400 (91.7491 iter/s, 2.17986s/200 iters), loss = 0.398394
I0315 05:41:13.303692  4128 solver.cpp:238]     Train net output #0: loss = 0.398394 (* 1 = 0.398394 loss)
I0315 05:41:13.303699  4128 sgd_solver.cpp:105] Iteration 40400, lr = 0.0005
I0315 05:41:14.362442  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:15.533798  4128 solver.cpp:219] Iteration 40600 (89.6871 iter/s, 2.22997s/200 iters), loss = 0.475544
I0315 05:41:15.533848  4128 solver.cpp:238]     Train net output #0: loss = 0.475545 (* 1 = 0.475545 loss)
I0315 05:41:15.533854  4128 sgd_solver.cpp:105] Iteration 40600, lr = 0.0005
I0315 05:41:17.161084  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:17.764780  4128 solver.cpp:219] Iteration 40800 (89.6524 iter/s, 2.23084s/200 iters), loss = 0.2812
I0315 05:41:17.764819  4128 solver.cpp:238]     Train net output #0: loss = 0.2812 (* 1 = 0.2812 loss)
I0315 05:41:17.764825  4128 sgd_solver.cpp:105] Iteration 40800, lr = 0.0005
I0315 05:41:19.930658  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:19.930958  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:19.965617  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_41000.caffemodel.h5
I0315 05:41:19.971650  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_41000.solverstate.h5
I0315 05:41:19.972362  4128 solver.cpp:332] Iteration 41000, Testing net (#0)
I0315 05:41:19.972373  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:41:19.972374  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:41:19.972376  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:41:19.972378  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:41:20.136821  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:41:20.597014  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7543
I0315 05:41:20.597041  4128 solver.cpp:399]     Test net output #1: loss = 0.723056 (* 1 = 0.723056 loss)
I0315 05:41:20.607077  4128 solver.cpp:219] Iteration 41000 (70.3696 iter/s, 2.84214s/200 iters), loss = 0.37605
I0315 05:41:20.607123  4128 solver.cpp:238]     Train net output #0: loss = 0.37605 (* 1 = 0.37605 loss)
I0315 05:41:20.607128  4128 sgd_solver.cpp:105] Iteration 41000, lr = 0.0005
I0315 05:41:22.798809  4128 solver.cpp:219] Iteration 41200 (91.258 iter/s, 2.19159s/200 iters), loss = 0.308936
I0315 05:41:22.798868  4128 solver.cpp:238]     Train net output #0: loss = 0.308936 (* 1 = 0.308936 loss)
I0315 05:41:22.798877  4128 sgd_solver.cpp:105] Iteration 41200, lr = 0.0005
I0315 05:41:23.307912  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:25.033186  4128 solver.cpp:219] Iteration 41400 (89.5164 iter/s, 2.23423s/200 iters), loss = 0.396865
I0315 05:41:25.033224  4128 solver.cpp:238]     Train net output #0: loss = 0.396865 (* 1 = 0.396865 loss)
I0315 05:41:25.033229  4128 sgd_solver.cpp:105] Iteration 41400, lr = 0.0005
I0315 05:41:26.100297  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:27.270205  4128 solver.cpp:219] Iteration 41600 (89.4102 iter/s, 2.23688s/200 iters), loss = 0.472814
I0315 05:41:27.270254  4128 solver.cpp:238]     Train net output #0: loss = 0.472814 (* 1 = 0.472814 loss)
I0315 05:41:27.270261  4128 sgd_solver.cpp:105] Iteration 41600, lr = 0.0005
I0315 05:41:28.892832  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:29.500207  4128 solver.cpp:219] Iteration 41800 (89.6921 iter/s, 2.22985s/200 iters), loss = 0.278696
I0315 05:41:29.500267  4128 solver.cpp:238]     Train net output #0: loss = 0.278696 (* 1 = 0.278696 loss)
I0315 05:41:29.500310  4128 sgd_solver.cpp:105] Iteration 41800, lr = 0.0005
I0315 05:41:31.619879  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:31.620162  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:31.652882  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_42000.caffemodel.h5
I0315 05:41:31.658988  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_42000.solverstate.h5
I0315 05:41:31.659747  4128 solver.cpp:332] Iteration 42000, Testing net (#0)
I0315 05:41:31.659756  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:41:31.659759  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:41:31.659760  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:41:31.659761  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:41:31.755971  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:41:32.294709  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7544
I0315 05:41:32.294737  4128 solver.cpp:399]     Test net output #1: loss = 0.721799 (* 1 = 0.721799 loss)
I0315 05:41:32.305377  4128 solver.cpp:219] Iteration 42000 (71.3014 iter/s, 2.80499s/200 iters), loss = 0.373534
I0315 05:41:32.305421  4128 solver.cpp:238]     Train net output #0: loss = 0.373534 (* 1 = 0.373534 loss)
I0315 05:41:32.305428  4128 sgd_solver.cpp:105] Iteration 42000, lr = 0.0005
I0315 05:41:34.554836  4128 solver.cpp:219] Iteration 42200 (88.9159 iter/s, 2.24932s/200 iters), loss = 0.305132
I0315 05:41:34.554886  4128 solver.cpp:238]     Train net output #0: loss = 0.305132 (* 1 = 0.305132 loss)
I0315 05:41:34.554893  4128 sgd_solver.cpp:105] Iteration 42200, lr = 0.0005
I0315 05:41:35.062664  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:36.781469  4128 solver.cpp:219] Iteration 42400 (89.8277 iter/s, 2.22648s/200 iters), loss = 0.394829
I0315 05:41:36.781512  4128 solver.cpp:238]     Train net output #0: loss = 0.394829 (* 1 = 0.394829 loss)
I0315 05:41:36.781518  4128 sgd_solver.cpp:105] Iteration 42400, lr = 0.0005
I0315 05:41:37.787631  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:38.949072  4128 solver.cpp:219] Iteration 42600 (92.274 iter/s, 2.16746s/200 iters), loss = 0.469334
I0315 05:41:38.949126  4128 solver.cpp:238]     Train net output #0: loss = 0.469334 (* 1 = 0.469334 loss)
I0315 05:41:38.949133  4128 sgd_solver.cpp:105] Iteration 42600, lr = 0.0005
I0315 05:41:40.576807  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:41.178201  4128 solver.cpp:219] Iteration 42800 (89.727 iter/s, 2.22898s/200 iters), loss = 0.278078
I0315 05:41:41.178493  4128 solver.cpp:238]     Train net output #0: loss = 0.278079 (* 1 = 0.278079 loss)
I0315 05:41:41.178500  4128 sgd_solver.cpp:105] Iteration 42800, lr = 0.0005
I0315 05:41:43.370715  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:43.371040  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:43.404140  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_43000.caffemodel.h5
I0315 05:41:43.410127  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_43000.solverstate.h5
I0315 05:41:43.410847  4128 solver.cpp:332] Iteration 43000, Testing net (#0)
I0315 05:41:43.410856  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:41:43.410859  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:41:43.410861  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:41:43.410862  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:41:43.555418  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:41:44.037276  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7549
I0315 05:41:44.037303  4128 solver.cpp:399]     Test net output #1: loss = 0.720803 (* 1 = 0.720803 loss)
I0315 05:41:44.047449  4128 solver.cpp:219] Iteration 43000 (69.7147 iter/s, 2.86883s/200 iters), loss = 0.371104
I0315 05:41:44.047497  4128 solver.cpp:238]     Train net output #0: loss = 0.371104 (* 1 = 0.371104 loss)
I0315 05:41:44.047503  4128 sgd_solver.cpp:105] Iteration 43000, lr = 0.0005
I0315 05:41:46.252116  4128 solver.cpp:219] Iteration 43200 (90.7226 iter/s, 2.20452s/200 iters), loss = 0.305211
I0315 05:41:46.252163  4128 solver.cpp:238]     Train net output #0: loss = 0.305211 (* 1 = 0.305211 loss)
I0315 05:41:46.252171  4128 sgd_solver.cpp:105] Iteration 43200, lr = 0.0005
I0315 05:41:46.719229  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:48.437933  4128 solver.cpp:219] Iteration 43400 (91.5935 iter/s, 2.18356s/200 iters), loss = 0.392015
I0315 05:41:48.437989  4128 solver.cpp:238]     Train net output #0: loss = 0.392015 (* 1 = 0.392015 loss)
I0315 05:41:48.437997  4128 sgd_solver.cpp:105] Iteration 43400, lr = 0.0005
I0315 05:41:49.494143  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:50.655302  4128 solver.cpp:219] Iteration 43600 (90.2031 iter/s, 2.21722s/200 iters), loss = 0.466561
I0315 05:41:50.655355  4128 solver.cpp:238]     Train net output #0: loss = 0.466561 (* 1 = 0.466561 loss)
I0315 05:41:50.655362  4128 sgd_solver.cpp:105] Iteration 43600, lr = 0.0005
I0315 05:41:52.241446  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:52.889329  4128 solver.cpp:219] Iteration 43800 (89.5308 iter/s, 2.23387s/200 iters), loss = 0.276651
I0315 05:41:52.889397  4128 solver.cpp:238]     Train net output #0: loss = 0.276652 (* 1 = 0.276652 loss)
I0315 05:41:52.889407  4128 sgd_solver.cpp:105] Iteration 43800, lr = 0.0005
I0315 05:41:55.018543  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:55.025193  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:41:55.052198  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_44000.caffemodel.h5
I0315 05:41:55.057303  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_44000.solverstate.h5
I0315 05:41:55.058277  4128 solver.cpp:332] Iteration 44000, Testing net (#0)
I0315 05:41:55.058316  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:41:55.058329  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:41:55.058338  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:41:55.058346  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:41:55.245822  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:41:55.702929  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7553
I0315 05:41:55.702960  4128 solver.cpp:399]     Test net output #1: loss = 0.720412 (* 1 = 0.720412 loss)
I0315 05:41:55.712779  4128 solver.cpp:219] Iteration 44000 (70.8401 iter/s, 2.82326s/200 iters), loss = 0.368255
I0315 05:41:55.713003  4128 solver.cpp:238]     Train net output #0: loss = 0.368255 (* 1 = 0.368255 loss)
I0315 05:41:55.713016  4128 sgd_solver.cpp:105] Iteration 44000, lr = 0.0005
I0315 05:41:57.959448  4128 solver.cpp:219] Iteration 44200 (89.033 iter/s, 2.24636s/200 iters), loss = 0.303311
I0315 05:41:57.959494  4128 solver.cpp:238]     Train net output #0: loss = 0.303311 (* 1 = 0.303311 loss)
I0315 05:41:57.959501  4128 sgd_solver.cpp:105] Iteration 44200, lr = 0.0005
I0315 05:41:58.462177  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:00.182181  4128 solver.cpp:219] Iteration 44400 (89.9851 iter/s, 2.22259s/200 iters), loss = 0.388962
I0315 05:42:00.182224  4128 solver.cpp:238]     Train net output #0: loss = 0.388962 (* 1 = 0.388962 loss)
I0315 05:42:00.182229  4128 sgd_solver.cpp:105] Iteration 44400, lr = 0.0005
I0315 05:42:01.255487  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:02.418475  4128 solver.cpp:219] Iteration 44600 (89.4391 iter/s, 2.23616s/200 iters), loss = 0.465183
I0315 05:42:02.418520  4128 solver.cpp:238]     Train net output #0: loss = 0.465183 (* 1 = 0.465183 loss)
I0315 05:42:02.418526  4128 sgd_solver.cpp:105] Iteration 44600, lr = 0.0005
I0315 05:42:03.985899  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:04.625375  4128 solver.cpp:219] Iteration 44800 (90.6307 iter/s, 2.20676s/200 iters), loss = 0.274428
I0315 05:42:04.625423  4128 solver.cpp:238]     Train net output #0: loss = 0.274428 (* 1 = 0.274428 loss)
I0315 05:42:04.625429  4128 sgd_solver.cpp:105] Iteration 44800, lr = 0.0005
I0315 05:42:06.773141  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:06.775437  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:06.810364  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_45000.caffemodel.h5
I0315 05:42:06.816573  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_45000.solverstate.h5
I0315 05:42:06.817510  4128 solver.cpp:332] Iteration 45000, Testing net (#0)
I0315 05:42:06.817524  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:42:06.817526  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:42:06.817528  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:42:06.817530  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:42:06.970072  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:42:07.432991  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7555
I0315 05:42:07.433024  4128 solver.cpp:399]     Test net output #1: loss = 0.719552 (* 1 = 0.719552 loss)
I0315 05:42:07.466997  4128 solver.cpp:219] Iteration 45000 (70.4394 iter/s, 2.83932s/200 iters), loss = 0.365402
I0315 05:42:07.467046  4128 solver.cpp:238]     Train net output #0: loss = 0.365402 (* 1 = 0.365402 loss)
I0315 05:42:07.467051  4128 sgd_solver.cpp:105] Iteration 45000, lr = 0.0005
I0315 05:42:09.653785  4128 solver.cpp:219] Iteration 45200 (91.553 iter/s, 2.18453s/200 iters), loss = 0.301234
I0315 05:42:09.653833  4128 solver.cpp:238]     Train net output #0: loss = 0.301234 (* 1 = 0.301234 loss)
I0315 05:42:09.653838  4128 sgd_solver.cpp:105] Iteration 45200, lr = 0.0005
I0315 05:42:10.174746  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:11.908030  4128 solver.cpp:219] Iteration 45400 (88.7272 iter/s, 2.2541s/200 iters), loss = 0.3874
I0315 05:42:11.909824  4128 solver.cpp:238]     Train net output #0: loss = 0.3874 (* 1 = 0.3874 loss)
I0315 05:42:11.909832  4128 sgd_solver.cpp:105] Iteration 45400, lr = 0.0005
I0315 05:42:12.981741  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:14.148727  4128 solver.cpp:219] Iteration 45600 (89.3332 iter/s, 2.23881s/200 iters), loss = 0.461654
I0315 05:42:14.148782  4128 solver.cpp:238]     Train net output #0: loss = 0.461654 (* 1 = 0.461654 loss)
I0315 05:42:14.148789  4128 sgd_solver.cpp:105] Iteration 45600, lr = 0.0005
I0315 05:42:15.739547  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:16.342864  4128 solver.cpp:219] Iteration 45800 (91.1585 iter/s, 2.19398s/200 iters), loss = 0.27174
I0315 05:42:16.343127  4128 solver.cpp:238]     Train net output #0: loss = 0.27174 (* 1 = 0.27174 loss)
I0315 05:42:16.343214  4128 sgd_solver.cpp:105] Iteration 45800, lr = 0.0005
I0315 05:42:18.531803  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:18.532400  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:18.566347  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_46000.caffemodel.h5
I0315 05:42:18.572409  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_46000.solverstate.h5
I0315 05:42:18.573138  4128 solver.cpp:332] Iteration 46000, Testing net (#0)
I0315 05:42:18.573150  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:42:18.573153  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:42:18.573154  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:42:18.573156  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:42:18.672951  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:42:19.226042  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7556
I0315 05:42:19.226079  4128 solver.cpp:399]     Test net output #1: loss = 0.718754 (* 1 = 0.718754 loss)
I0315 05:42:19.252609  4128 solver.cpp:219] Iteration 46000 (68.7436 iter/s, 2.90936s/200 iters), loss = 0.36207
I0315 05:42:19.252655  4128 solver.cpp:238]     Train net output #0: loss = 0.362071 (* 1 = 0.362071 loss)
I0315 05:42:19.252661  4128 sgd_solver.cpp:105] Iteration 46000, lr = 0.0005
I0315 05:42:21.430867  4128 solver.cpp:219] Iteration 46200 (91.8244 iter/s, 2.17807s/200 iters), loss = 0.299115
I0315 05:42:21.430912  4128 solver.cpp:238]     Train net output #0: loss = 0.299115 (* 1 = 0.299115 loss)
I0315 05:42:21.430918  4128 sgd_solver.cpp:105] Iteration 46200, lr = 0.0005
I0315 05:42:21.939116  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:23.674927  4128 solver.cpp:219] Iteration 46400 (89.1298 iter/s, 2.24392s/200 iters), loss = 0.383893
I0315 05:42:23.674971  4128 solver.cpp:238]     Train net output #0: loss = 0.383893 (* 1 = 0.383893 loss)
I0315 05:42:23.674978  4128 sgd_solver.cpp:105] Iteration 46400, lr = 0.0005
I0315 05:42:24.741627  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:25.905290  4128 solver.cpp:219] Iteration 46600 (89.6772 iter/s, 2.23022s/200 iters), loss = 0.459551
I0315 05:42:25.905338  4128 solver.cpp:238]     Train net output #0: loss = 0.459551 (* 1 = 0.459551 loss)
I0315 05:42:25.905344  4128 sgd_solver.cpp:105] Iteration 46600, lr = 0.0005
I0315 05:42:27.477830  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:28.089083  4128 solver.cpp:219] Iteration 46800 (91.5896 iter/s, 2.18365s/200 iters), loss = 0.270296
I0315 05:42:28.089124  4128 solver.cpp:238]     Train net output #0: loss = 0.270296 (* 1 = 0.270296 loss)
I0315 05:42:28.089130  4128 sgd_solver.cpp:105] Iteration 46800, lr = 0.0005
I0315 05:42:30.280742  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:30.281136  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:30.313877  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_47000.caffemodel.h5
I0315 05:42:30.320183  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_47000.solverstate.h5
I0315 05:42:30.320966  4128 solver.cpp:332] Iteration 47000, Testing net (#0)
I0315 05:42:30.320978  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:42:30.320981  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:42:30.320982  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:42:30.320984  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:42:30.412024  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:42:30.949313  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7566
I0315 05:42:30.949339  4128 solver.cpp:399]     Test net output #1: loss = 0.717526 (* 1 = 0.717526 loss)
I0315 05:42:30.959306  4128 solver.cpp:219] Iteration 47000 (69.686 iter/s, 2.87002s/200 iters), loss = 0.359642
I0315 05:42:30.959352  4128 solver.cpp:238]     Train net output #0: loss = 0.359642 (* 1 = 0.359642 loss)
I0315 05:42:30.959358  4128 sgd_solver.cpp:105] Iteration 47000, lr = 0.0005
I0315 05:42:33.196442  4128 solver.cpp:219] Iteration 47200 (89.406 iter/s, 2.23699s/200 iters), loss = 0.297545
I0315 05:42:33.196499  4128 solver.cpp:238]     Train net output #0: loss = 0.297545 (* 1 = 0.297545 loss)
I0315 05:42:33.196506  4128 sgd_solver.cpp:105] Iteration 47200, lr = 0.0005
I0315 05:42:33.705343  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:35.434854  4128 solver.cpp:219] Iteration 47400 (89.3551 iter/s, 2.23826s/200 iters), loss = 0.380569
I0315 05:42:35.434895  4128 solver.cpp:238]     Train net output #0: loss = 0.380569 (* 1 = 0.380569 loss)
I0315 05:42:35.434900  4128 sgd_solver.cpp:105] Iteration 47400, lr = 0.0005
I0315 05:42:36.493005  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:37.601881  4128 solver.cpp:219] Iteration 47600 (92.2981 iter/s, 2.16689s/200 iters), loss = 0.457996
I0315 05:42:37.601925  4128 solver.cpp:238]     Train net output #0: loss = 0.457997 (* 1 = 0.457997 loss)
I0315 05:42:37.601932  4128 sgd_solver.cpp:105] Iteration 47600, lr = 0.0005
I0315 05:42:39.225306  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:39.839720  4128 solver.cpp:219] Iteration 47800 (89.3775 iter/s, 2.2377s/200 iters), loss = 0.26866
I0315 05:42:39.839766  4128 solver.cpp:238]     Train net output #0: loss = 0.268661 (* 1 = 0.268661 loss)
I0315 05:42:39.839771  4128 sgd_solver.cpp:105] Iteration 47800, lr = 0.0005
I0315 05:42:42.025028  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:42.025393  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:42.058759  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_48000.caffemodel.h5
I0315 05:42:42.064956  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_48000.solverstate.h5
I0315 05:42:42.065685  4128 solver.cpp:332] Iteration 48000, Testing net (#0)
I0315 05:42:42.065696  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:42:42.065697  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:42:42.065699  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:42:42.065701  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:42:42.203482  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:42:42.701714  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7567
I0315 05:42:42.701740  4128 solver.cpp:399]     Test net output #1: loss = 0.716671 (* 1 = 0.716671 loss)
I0315 05:42:42.711645  4128 solver.cpp:219] Iteration 48000 (69.6437 iter/s, 2.87176s/200 iters), loss = 0.357309
I0315 05:42:42.711694  4128 solver.cpp:238]     Train net output #0: loss = 0.357309 (* 1 = 0.357309 loss)
I0315 05:42:42.711701  4128 sgd_solver.cpp:105] Iteration 48000, lr = 0.0005
I0315 05:42:44.947542  4128 solver.cpp:219] Iteration 48200 (89.4554 iter/s, 2.23575s/200 iters), loss = 0.297178
I0315 05:42:44.947587  4128 solver.cpp:238]     Train net output #0: loss = 0.297179 (* 1 = 0.297179 loss)
I0315 05:42:44.947593  4128 sgd_solver.cpp:105] Iteration 48200, lr = 0.0005
I0315 05:42:45.463366  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:47.183248  4128 solver.cpp:219] Iteration 48400 (89.4629 iter/s, 2.23556s/200 iters), loss = 0.378794
I0315 05:42:47.183300  4128 solver.cpp:238]     Train net output #0: loss = 0.378795 (* 1 = 0.378795 loss)
I0315 05:42:47.183307  4128 sgd_solver.cpp:105] Iteration 48400, lr = 0.0005
I0315 05:42:48.198869  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:49.365533  4128 solver.cpp:219] Iteration 48600 (91.6566 iter/s, 2.18206s/200 iters), loss = 0.456206
I0315 05:42:49.365643  4128 solver.cpp:238]     Train net output #0: loss = 0.456207 (* 1 = 0.456207 loss)
I0315 05:42:49.365663  4128 sgd_solver.cpp:105] Iteration 48600, lr = 0.0005
I0315 05:42:50.992935  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:51.597708  4128 solver.cpp:219] Iteration 48800 (89.607 iter/s, 2.23197s/200 iters), loss = 0.267352
I0315 05:42:51.597760  4128 solver.cpp:238]     Train net output #0: loss = 0.267352 (* 1 = 0.267352 loss)
I0315 05:42:51.597765  4128 sgd_solver.cpp:105] Iteration 48800, lr = 0.0005
I0315 05:42:53.726450  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:53.727429  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:53.817142  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_49000.caffemodel.h5
I0315 05:42:53.823143  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_49000.solverstate.h5
I0315 05:42:53.823863  4128 solver.cpp:332] Iteration 49000, Testing net (#0)
I0315 05:42:53.823873  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:42:53.823874  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:42:53.823876  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:42:53.823879  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:42:53.910917  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:42:54.445420  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7559
I0315 05:42:54.445444  4128 solver.cpp:399]     Test net output #1: loss = 0.71636 (* 1 = 0.71636 loss)
I0315 05:42:54.455262  4128 solver.cpp:219] Iteration 49000 (69.9941 iter/s, 2.85738s/200 iters), loss = 0.354646
I0315 05:42:54.455307  4128 solver.cpp:238]     Train net output #0: loss = 0.354646 (* 1 = 0.354646 loss)
I0315 05:42:54.455312  4128 sgd_solver.cpp:105] Iteration 49000, lr = 0.0005
I0315 05:42:56.656214  4128 solver.cpp:219] Iteration 49200 (90.8755 iter/s, 2.20081s/200 iters), loss = 0.296002
I0315 05:42:56.656262  4128 solver.cpp:238]     Train net output #0: loss = 0.296002 (* 1 = 0.296002 loss)
I0315 05:42:56.656268  4128 sgd_solver.cpp:105] Iteration 49200, lr = 0.0005
I0315 05:42:57.172123  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:42:58.894047  4128 solver.cpp:219] Iteration 49400 (89.3781 iter/s, 2.23768s/200 iters), loss = 0.37589
I0315 05:42:58.894101  4128 solver.cpp:238]     Train net output #0: loss = 0.37589 (* 1 = 0.37589 loss)
I0315 05:42:58.894109  4128 sgd_solver.cpp:105] Iteration 49400, lr = 0.0005
I0315 05:42:59.972312  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:01.136644  4128 solver.cpp:219] Iteration 49600 (89.1881 iter/s, 2.24245s/200 iters), loss = 0.453834
I0315 05:43:01.136693  4128 solver.cpp:238]     Train net output #0: loss = 0.453835 (* 1 = 0.453835 loss)
I0315 05:43:01.136703  4128 sgd_solver.cpp:105] Iteration 49600, lr = 0.0005
I0315 05:43:02.768749  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:03.374182  4128 solver.cpp:219] Iteration 49800 (89.3896 iter/s, 2.2374s/200 iters), loss = 0.265735
I0315 05:43:03.374234  4128 solver.cpp:238]     Train net output #0: loss = 0.265735 (* 1 = 0.265735 loss)
I0315 05:43:03.374239  4128 sgd_solver.cpp:105] Iteration 49800, lr = 0.0005
I0315 05:43:05.504134  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:05.504581  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:05.537289  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_50000.caffemodel.h5
I0315 05:43:05.543396  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_50000.solverstate.h5
I0315 05:43:05.544122  4128 solver.cpp:332] Iteration 50000, Testing net (#0)
I0315 05:43:05.544131  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:43:05.544133  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:43:05.544136  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:43:05.544137  4128 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:43:06.188942  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7576
I0315 05:43:06.188966  4128 solver.cpp:399]     Test net output #1: loss = 0.715533 (* 1 = 0.715533 loss)
I0315 05:43:06.199167  4128 solver.cpp:219] Iteration 50000 (70.8012 iter/s, 2.82481s/200 iters), loss = 0.353048
I0315 05:43:06.199223  4128 solver.cpp:238]     Train net output #0: loss = 0.353048 (* 1 = 0.353048 loss)
I0315 05:43:06.199231  4128 sgd_solver.cpp:105] Iteration 50000, lr = 0.0005
I0315 05:43:06.249272  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:08.433336  4128 solver.cpp:219] Iteration 50200 (89.5248 iter/s, 2.23402s/200 iters), loss = 0.294436
I0315 05:43:08.433382  4128 solver.cpp:238]     Train net output #0: loss = 0.294436 (* 1 = 0.294436 loss)
I0315 05:43:08.433387  4128 sgd_solver.cpp:105] Iteration 50200, lr = 0.0005
I0315 05:43:08.943251  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:10.675271  4128 solver.cpp:219] Iteration 50400 (89.2145 iter/s, 2.24179s/200 iters), loss = 0.3724
I0315 05:43:10.675334  4128 solver.cpp:238]     Train net output #0: loss = 0.3724 (* 1 = 0.3724 loss)
I0315 05:43:10.675341  4128 sgd_solver.cpp:105] Iteration 50400, lr = 0.0005
I0315 05:43:11.742642  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:12.904505  4128 solver.cpp:219] Iteration 50600 (89.7234 iter/s, 2.22907s/200 iters), loss = 0.450745
I0315 05:43:12.904758  4128 solver.cpp:238]     Train net output #0: loss = 0.450745 (* 1 = 0.450745 loss)
I0315 05:43:12.904769  4128 sgd_solver.cpp:105] Iteration 50600, lr = 0.0005
I0315 05:43:14.473084  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:15.097959  4128 solver.cpp:219] Iteration 50800 (91.2001 iter/s, 2.19298s/200 iters), loss = 0.264343
I0315 05:43:15.098000  4128 solver.cpp:238]     Train net output #0: loss = 0.264344 (* 1 = 0.264344 loss)
I0315 05:43:15.098004  4128 sgd_solver.cpp:105] Iteration 50800, lr = 0.0005
I0315 05:43:17.259161  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:17.259454  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:17.293342  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_51000.caffemodel.h5
I0315 05:43:17.299439  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_51000.solverstate.h5
I0315 05:43:17.300165  4128 solver.cpp:332] Iteration 51000, Testing net (#0)
I0315 05:43:17.300176  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:43:17.300179  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:43:17.300179  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:43:17.300181  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:43:17.458317  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:43:17.927629  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7581
I0315 05:43:17.927654  4128 solver.cpp:399]     Test net output #1: loss = 0.714794 (* 1 = 0.714794 loss)
I0315 05:43:17.937331  4128 solver.cpp:219] Iteration 51000 (70.4947 iter/s, 2.83709s/200 iters), loss = 0.351313
I0315 05:43:17.937378  4128 solver.cpp:238]     Train net output #0: loss = 0.351313 (* 1 = 0.351313 loss)
I0315 05:43:17.937383  4128 sgd_solver.cpp:105] Iteration 51000, lr = 0.0005
I0315 05:43:20.180543  4128 solver.cpp:219] Iteration 51200 (89.1636 iter/s, 2.24307s/200 iters), loss = 0.293415
I0315 05:43:20.180593  4128 solver.cpp:238]     Train net output #0: loss = 0.293415 (* 1 = 0.293415 loss)
I0315 05:43:20.180598  4128 sgd_solver.cpp:105] Iteration 51200, lr = 0.0005
I0315 05:43:20.686775  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:22.346432  4128 solver.cpp:219] Iteration 51400 (92.3476 iter/s, 2.16573s/200 iters), loss = 0.370559
I0315 05:43:22.346493  4128 solver.cpp:238]     Train net output #0: loss = 0.370559 (* 1 = 0.370559 loss)
I0315 05:43:22.346499  4128 sgd_solver.cpp:105] Iteration 51400, lr = 0.0005
I0315 05:43:23.406132  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:24.573957  4128 solver.cpp:219] Iteration 51600 (89.7922 iter/s, 2.22736s/200 iters), loss = 0.449087
I0315 05:43:24.574020  4128 solver.cpp:238]     Train net output #0: loss = 0.449088 (* 1 = 0.449088 loss)
I0315 05:43:24.574029  4128 sgd_solver.cpp:105] Iteration 51600, lr = 0.0005
I0315 05:43:26.216850  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:26.822134  4128 solver.cpp:219] Iteration 51800 (88.967 iter/s, 2.24802s/200 iters), loss = 0.262694
I0315 05:43:26.822183  4128 solver.cpp:238]     Train net output #0: loss = 0.262695 (* 1 = 0.262695 loss)
I0315 05:43:26.822190  4128 sgd_solver.cpp:105] Iteration 51800, lr = 0.0005
I0315 05:43:29.005071  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:29.005424  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:29.038815  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_52000.caffemodel.h5
I0315 05:43:29.044855  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_52000.solverstate.h5
I0315 05:43:29.045593  4128 solver.cpp:332] Iteration 52000, Testing net (#0)
I0315 05:43:29.045604  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:43:29.045608  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:43:29.045608  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:43:29.045773  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:43:29.130728  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:43:29.682221  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7584
I0315 05:43:29.682245  4128 solver.cpp:399]     Test net output #1: loss = 0.714267 (* 1 = 0.714267 loss)
I0315 05:43:29.692468  4128 solver.cpp:219] Iteration 52000 (69.6822 iter/s, 2.87017s/200 iters), loss = 0.348657
I0315 05:43:29.692512  4128 solver.cpp:238]     Train net output #0: loss = 0.348657 (* 1 = 0.348657 loss)
I0315 05:43:29.692517  4128 sgd_solver.cpp:105] Iteration 52000, lr = 0.0005
I0315 05:43:31.944413  4128 solver.cpp:219] Iteration 52200 (88.8176 iter/s, 2.25181s/200 iters), loss = 0.290258
I0315 05:43:31.944466  4128 solver.cpp:238]     Train net output #0: loss = 0.290258 (* 1 = 0.290258 loss)
I0315 05:43:31.944473  4128 sgd_solver.cpp:105] Iteration 52200, lr = 0.0005
I0315 05:43:32.400923  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:34.124934  4128 solver.cpp:219] Iteration 52400 (91.7272 iter/s, 2.18038s/200 iters), loss = 0.369386
I0315 05:43:34.124985  4128 solver.cpp:238]     Train net output #0: loss = 0.369386 (* 1 = 0.369386 loss)
I0315 05:43:34.124991  4128 sgd_solver.cpp:105] Iteration 52400, lr = 0.0005
I0315 05:43:35.192952  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:36.371281  4128 solver.cpp:219] Iteration 52600 (89.0393 iter/s, 2.2462s/200 iters), loss = 0.447907
I0315 05:43:36.371330  4128 solver.cpp:238]     Train net output #0: loss = 0.447907 (* 1 = 0.447907 loss)
I0315 05:43:36.371335  4128 sgd_solver.cpp:105] Iteration 52600, lr = 0.0005
I0315 05:43:38.007974  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:38.624650  4128 solver.cpp:219] Iteration 52800 (88.7616 iter/s, 2.25323s/200 iters), loss = 0.260851
I0315 05:43:38.624697  4128 solver.cpp:238]     Train net output #0: loss = 0.260851 (* 1 = 0.260851 loss)
I0315 05:43:38.624703  4128 sgd_solver.cpp:105] Iteration 52800, lr = 0.0005
I0315 05:43:40.827258  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:40.827800  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:40.861124  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_53000.caffemodel.h5
I0315 05:43:40.867202  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_53000.solverstate.h5
I0315 05:43:40.867931  4128 solver.cpp:332] Iteration 53000, Testing net (#0)
I0315 05:43:40.867944  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:43:40.867946  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:43:40.867947  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:43:40.867949  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:43:41.025048  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:43:41.498561  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7586
I0315 05:43:41.498589  4128 solver.cpp:399]     Test net output #1: loss = 0.713571 (* 1 = 0.713571 loss)
I0315 05:43:41.508328  4128 solver.cpp:219] Iteration 53000 (69.3624 iter/s, 2.88341s/200 iters), loss = 0.345781
I0315 05:43:41.508385  4128 solver.cpp:238]     Train net output #0: loss = 0.345781 (* 1 = 0.345781 loss)
I0315 05:43:41.508394  4128 sgd_solver.cpp:105] Iteration 53000, lr = 0.0005
I0315 05:43:43.751018  4128 solver.cpp:219] Iteration 53200 (89.1848 iter/s, 2.24254s/200 iters), loss = 0.289412
I0315 05:43:43.751202  4128 solver.cpp:238]     Train net output #0: loss = 0.289413 (* 1 = 0.289413 loss)
I0315 05:43:43.751214  4128 sgd_solver.cpp:105] Iteration 53200, lr = 0.0005
I0315 05:43:44.198896  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:45.929901  4128 solver.cpp:219] Iteration 53400 (91.8019 iter/s, 2.17861s/200 iters), loss = 0.367536
I0315 05:43:45.929970  4128 solver.cpp:238]     Train net output #0: loss = 0.367537 (* 1 = 0.367537 loss)
I0315 05:43:45.929980  4128 sgd_solver.cpp:105] Iteration 53400, lr = 0.0005
I0315 05:43:46.998919  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:48.176720  4128 solver.cpp:219] Iteration 53600 (89.021 iter/s, 2.24666s/200 iters), loss = 0.445201
I0315 05:43:48.176765  4128 solver.cpp:238]     Train net output #0: loss = 0.445201 (* 1 = 0.445201 loss)
I0315 05:43:48.176771  4128 sgd_solver.cpp:105] Iteration 53600, lr = 0.0005
I0315 05:43:49.790227  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:50.375588  4128 solver.cpp:219] Iteration 53800 (90.9617 iter/s, 2.19873s/200 iters), loss = 0.259365
I0315 05:43:50.375643  4128 solver.cpp:238]     Train net output #0: loss = 0.259365 (* 1 = 0.259365 loss)
I0315 05:43:50.375650  4128 sgd_solver.cpp:105] Iteration 53800, lr = 0.0005
I0315 05:43:52.548297  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:52.548745  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:52.582321  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_54000.caffemodel.h5
I0315 05:43:52.590273  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_54000.solverstate.h5
I0315 05:43:52.591140  4128 solver.cpp:332] Iteration 54000, Testing net (#0)
I0315 05:43:52.591178  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:43:52.591195  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:43:52.591204  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:43:52.591213  4128 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:43:53.226737  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7587
I0315 05:43:53.226764  4128 solver.cpp:399]     Test net output #1: loss = 0.712572 (* 1 = 0.712572 loss)
I0315 05:43:53.236553  4128 solver.cpp:219] Iteration 54000 (69.9107 iter/s, 2.86079s/200 iters), loss = 0.343974
I0315 05:43:53.236595  4128 solver.cpp:238]     Train net output #0: loss = 0.343974 (* 1 = 0.343974 loss)
I0315 05:43:53.236600  4128 sgd_solver.cpp:105] Iteration 54000, lr = 0.0005
I0315 05:43:53.343814  4178 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:55.480293  4128 solver.cpp:219] Iteration 54200 (89.1424 iter/s, 2.2436s/200 iters), loss = 0.288697
I0315 05:43:55.480341  4128 solver.cpp:238]     Train net output #0: loss = 0.288698 (* 1 = 0.288698 loss)
I0315 05:43:55.480347  4128 sgd_solver.cpp:105] Iteration 54200, lr = 0.0005
I0315 05:43:55.989019  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:57.717116  4128 solver.cpp:219] Iteration 54400 (89.4182 iter/s, 2.23668s/200 iters), loss = 0.364894
I0315 05:43:57.717164  4128 solver.cpp:238]     Train net output #0: loss = 0.364894 (* 1 = 0.364894 loss)
I0315 05:43:57.717169  4128 sgd_solver.cpp:105] Iteration 54400, lr = 0.0005
I0315 05:43:58.777235  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:43:59.944267  4128 solver.cpp:219] Iteration 54600 (89.8065 iter/s, 2.22701s/200 iters), loss = 0.442325
I0315 05:43:59.944314  4128 solver.cpp:238]     Train net output #0: loss = 0.442325 (* 1 = 0.442325 loss)
I0315 05:43:59.944320  4128 sgd_solver.cpp:105] Iteration 54600, lr = 0.0005
I0315 05:44:01.521584  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:02.140204  4128 solver.cpp:219] Iteration 54800 (91.083 iter/s, 2.1958s/200 iters), loss = 0.258278
I0315 05:44:02.140246  4128 solver.cpp:238]     Train net output #0: loss = 0.258278 (* 1 = 0.258278 loss)
I0315 05:44:02.140277  4128 sgd_solver.cpp:105] Iteration 54800, lr = 0.0005
I0315 05:44:04.341621  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:04.341938  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:04.375551  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_55000.caffemodel.h5
I0315 05:44:04.381759  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_55000.solverstate.h5
I0315 05:44:04.382799  4128 solver.cpp:332] Iteration 55000, Testing net (#0)
I0315 05:44:04.382812  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:04.382815  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:04.382818  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:04.382823  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:44:04.529011  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:44:05.011644  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7599
I0315 05:44:05.011669  4128 solver.cpp:399]     Test net output #1: loss = 0.711569 (* 1 = 0.711569 loss)
I0315 05:44:05.021754  4128 solver.cpp:219] Iteration 55000 (69.4109 iter/s, 2.88139s/200 iters), loss = 0.34311
I0315 05:44:05.021796  4128 solver.cpp:238]     Train net output #0: loss = 0.34311 (* 1 = 0.34311 loss)
I0315 05:44:05.021802  4128 sgd_solver.cpp:105] Iteration 55000, lr = 0.0005
I0315 05:44:07.259284  4128 solver.cpp:219] Iteration 55200 (89.3897 iter/s, 2.23739s/200 iters), loss = 0.288262
I0315 05:44:07.259330  4128 solver.cpp:238]     Train net output #0: loss = 0.288263 (* 1 = 0.288263 loss)
I0315 05:44:07.259335  4128 sgd_solver.cpp:105] Iteration 55200, lr = 0.0005
I0315 05:44:07.775132  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:09.516952  4128 solver.cpp:219] Iteration 55400 (88.5925 iter/s, 2.25753s/200 iters), loss = 0.364137
I0315 05:44:09.516999  4128 solver.cpp:238]     Train net output #0: loss = 0.364137 (* 1 = 0.364137 loss)
I0315 05:44:09.517005  4128 sgd_solver.cpp:105] Iteration 55400, lr = 0.0005
I0315 05:44:10.545418  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:11.729698  4128 solver.cpp:219] Iteration 55600 (90.3911 iter/s, 2.21261s/200 iters), loss = 0.44194
I0315 05:44:11.729742  4128 solver.cpp:238]     Train net output #0: loss = 0.44194 (* 1 = 0.44194 loss)
I0315 05:44:11.729748  4128 sgd_solver.cpp:105] Iteration 55600, lr = 0.0005
I0315 05:44:13.337203  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:13.940603  4128 solver.cpp:219] Iteration 55800 (90.553 iter/s, 2.20865s/200 iters), loss = 0.257633
I0315 05:44:13.940793  4128 solver.cpp:238]     Train net output #0: loss = 0.257633 (* 1 = 0.257633 loss)
I0315 05:44:13.940803  4128 sgd_solver.cpp:105] Iteration 55800, lr = 0.0005
I0315 05:44:16.114568  4154 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:16.114949  4153 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:16.147804  4128 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_56000.caffemodel.h5
I0315 05:44:16.154008  4128 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_56000.solverstate.h5
I0315 05:44:16.159165  4128 solver.cpp:312] Iteration 56000, loss = 0.341499
I0315 05:44:16.159195  4128 solver.cpp:332] Iteration 56000, Testing net (#0)
I0315 05:44:16.159199  4128 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:16.159201  4128 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:16.159202  4128 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:16.159204  4128 net.cpp:678] Ignoring source layer concat_data
I0315 05:44:16.247474  4178 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:44:16.784883  4128 solver.cpp:399]     Test net output #0: accuracy = 0.7601
I0315 05:44:16.784906  4128 solver.cpp:399]     Test net output #1: loss = 0.711287 (* 1 = 0.711287 loss)
I0315 05:44:16.784909  4128 solver.cpp:317] Optimization Done.
I0315 05:44:16.784912  4128 caffe.cpp:259] Optimization Done.
I0315 05:44:17.049772 12110 caffe.cpp:218] Using GPUs 2
I0315 05:44:17.078599 12110 caffe.cpp:223] GPU 2: TITAN X (Pascal)
I0315 05:44:17.446506 12110 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 65000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/random/models/"
solver_mode: GPU
device_id: 2
net: "examples/cifar10/random/cifar10_full_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_format: HDF5
I0315 05:44:17.446650 12110 solver.cpp:87] Creating training net from net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:44:17.447065 12110 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0315 05:44:17.447083 12110 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0315 05:44:17.447221 12110 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar1"
  type: "Data"
  top: "data_fixed"
  top: "label_fixed"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_20000"
    batch_size: 80
    backend: LMDB
  }
}
layer {
  name: "cifar2"
  type: "Data"
  top: "data_random"
  top: "label_random"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_random_10000"
    batch_size: 20
    backend: LMDB
  }
}
layer {
  name: "concat_label"
  type: "Concat"
  bottom: "label_fixed"
  bottom: "label_random"
  top: "label"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "concat_data"
  type: "Concat"
  bottom: "data_fixed"
  bottom: "data_random"
  top: "data"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:44:17.447314 12110 layer_factory.hpp:77] Creating layer cifar1
I0315 05:44:17.447412 12110 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_20000
I0315 05:44:17.447450 12110 net.cpp:86] Creating Layer cifar1
I0315 05:44:17.447459 12110 net.cpp:382] cifar1 -> data_fixed
I0315 05:44:17.447482 12110 net.cpp:382] cifar1 -> label_fixed
I0315 05:44:17.447499 12110 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:44:17.449857 12110 data_layer.cpp:45] output data size: 80,3,32,32
I0315 05:44:17.456223 12110 net.cpp:124] Setting up cifar1
I0315 05:44:17.456245 12110 net.cpp:131] Top shape: 80 3 32 32 (245760)
I0315 05:44:17.456249 12110 net.cpp:131] Top shape: 80 (80)
I0315 05:44:17.456251 12110 net.cpp:139] Memory required for data: 983360
I0315 05:44:17.456259 12110 layer_factory.hpp:77] Creating layer cifar2
I0315 05:44:17.456334 12110 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_random_10000
I0315 05:44:17.456349 12110 net.cpp:86] Creating Layer cifar2
I0315 05:44:17.456356 12110 net.cpp:382] cifar2 -> data_random
I0315 05:44:17.456365 12110 net.cpp:382] cifar2 -> label_random
I0315 05:44:17.456372 12110 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:44:17.456481 12110 data_layer.cpp:45] output data size: 20,3,32,32
I0315 05:44:17.458489 12110 net.cpp:124] Setting up cifar2
I0315 05:44:17.458508 12110 net.cpp:131] Top shape: 20 3 32 32 (61440)
I0315 05:44:17.458511 12110 net.cpp:131] Top shape: 20 (20)
I0315 05:44:17.458513 12110 net.cpp:139] Memory required for data: 1229200
I0315 05:44:17.458518 12110 layer_factory.hpp:77] Creating layer concat_label
I0315 05:44:17.458529 12110 net.cpp:86] Creating Layer concat_label
I0315 05:44:17.458534 12110 net.cpp:408] concat_label <- label_fixed
I0315 05:44:17.458544 12110 net.cpp:408] concat_label <- label_random
I0315 05:44:17.458549 12110 net.cpp:382] concat_label -> label
I0315 05:44:17.458582 12110 net.cpp:124] Setting up concat_label
I0315 05:44:17.458588 12110 net.cpp:131] Top shape: 100 (100)
I0315 05:44:17.458591 12110 net.cpp:139] Memory required for data: 1229600
I0315 05:44:17.458595 12110 layer_factory.hpp:77] Creating layer concat_data
I0315 05:44:17.458600 12110 net.cpp:86] Creating Layer concat_data
I0315 05:44:17.458605 12110 net.cpp:408] concat_data <- data_fixed
I0315 05:44:17.458608 12110 net.cpp:408] concat_data <- data_random
I0315 05:44:17.458613 12110 net.cpp:382] concat_data -> data
I0315 05:44:17.458650 12110 net.cpp:124] Setting up concat_data
I0315 05:44:17.458657 12110 net.cpp:131] Top shape: 100 3 32 32 (307200)
I0315 05:44:17.458660 12110 net.cpp:139] Memory required for data: 2458400
I0315 05:44:17.458664 12110 layer_factory.hpp:77] Creating layer conv1
I0315 05:44:17.458685 12110 net.cpp:86] Creating Layer conv1
I0315 05:44:17.458690 12110 net.cpp:408] conv1 <- data
I0315 05:44:17.458696 12110 net.cpp:382] conv1 -> conv1
I0315 05:44:18.044302 12110 net.cpp:124] Setting up conv1
I0315 05:44:18.044332 12110 net.cpp:131] Top shape: 100 32 32 32 (3276800)
I0315 05:44:18.044335 12110 net.cpp:139] Memory required for data: 15565600
I0315 05:44:18.044360 12110 layer_factory.hpp:77] Creating layer pool1
I0315 05:44:18.044375 12110 net.cpp:86] Creating Layer pool1
I0315 05:44:18.044381 12110 net.cpp:408] pool1 <- conv1
I0315 05:44:18.044389 12110 net.cpp:382] pool1 -> pool1
I0315 05:44:18.044456 12110 net.cpp:124] Setting up pool1
I0315 05:44:18.044466 12110 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:44:18.044468 12110 net.cpp:139] Memory required for data: 18842400
I0315 05:44:18.044473 12110 layer_factory.hpp:77] Creating layer relu1
I0315 05:44:18.044479 12110 net.cpp:86] Creating Layer relu1
I0315 05:44:18.044486 12110 net.cpp:408] relu1 <- pool1
I0315 05:44:18.044513 12110 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:44:18.044719 12110 net.cpp:124] Setting up relu1
I0315 05:44:18.044730 12110 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:44:18.044736 12110 net.cpp:139] Memory required for data: 22119200
I0315 05:44:18.044740 12110 layer_factory.hpp:77] Creating layer norm1
I0315 05:44:18.044750 12110 net.cpp:86] Creating Layer norm1
I0315 05:44:18.044755 12110 net.cpp:408] norm1 <- pool1
I0315 05:44:18.044764 12110 net.cpp:382] norm1 -> norm1
I0315 05:44:18.047056 12110 net.cpp:124] Setting up norm1
I0315 05:44:18.047080 12110 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:44:18.047083 12110 net.cpp:139] Memory required for data: 25396000
I0315 05:44:18.047088 12110 layer_factory.hpp:77] Creating layer conv2
I0315 05:44:18.047111 12110 net.cpp:86] Creating Layer conv2
I0315 05:44:18.047116 12110 net.cpp:408] conv2 <- norm1
I0315 05:44:18.047127 12110 net.cpp:382] conv2 -> conv2
I0315 05:44:18.048950 12110 net.cpp:124] Setting up conv2
I0315 05:44:18.048972 12110 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:44:18.048975 12110 net.cpp:139] Memory required for data: 28672800
I0315 05:44:18.048990 12110 layer_factory.hpp:77] Creating layer relu2
I0315 05:44:18.049001 12110 net.cpp:86] Creating Layer relu2
I0315 05:44:18.049006 12110 net.cpp:408] relu2 <- conv2
I0315 05:44:18.049015 12110 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:44:18.049823 12110 net.cpp:124] Setting up relu2
I0315 05:44:18.049837 12110 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:44:18.049842 12110 net.cpp:139] Memory required for data: 31949600
I0315 05:44:18.049846 12110 layer_factory.hpp:77] Creating layer pool2
I0315 05:44:18.049855 12110 net.cpp:86] Creating Layer pool2
I0315 05:44:18.049860 12110 net.cpp:408] pool2 <- conv2
I0315 05:44:18.049866 12110 net.cpp:382] pool2 -> pool2
I0315 05:44:18.050061 12110 net.cpp:124] Setting up pool2
I0315 05:44:18.050072 12110 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:44:18.050077 12110 net.cpp:139] Memory required for data: 32768800
I0315 05:44:18.050081 12110 layer_factory.hpp:77] Creating layer norm2
I0315 05:44:18.050091 12110 net.cpp:86] Creating Layer norm2
I0315 05:44:18.050096 12110 net.cpp:408] norm2 <- pool2
I0315 05:44:18.050103 12110 net.cpp:382] norm2 -> norm2
I0315 05:44:18.050376 12110 net.cpp:124] Setting up norm2
I0315 05:44:18.050387 12110 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:44:18.050392 12110 net.cpp:139] Memory required for data: 33588000
I0315 05:44:18.050405 12110 layer_factory.hpp:77] Creating layer conv3
I0315 05:44:18.050418 12110 net.cpp:86] Creating Layer conv3
I0315 05:44:18.050422 12110 net.cpp:408] conv3 <- norm2
I0315 05:44:18.050429 12110 net.cpp:382] conv3 -> conv3
I0315 05:44:18.052599 12110 net.cpp:124] Setting up conv3
I0315 05:44:18.052620 12110 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:44:18.052624 12110 net.cpp:139] Memory required for data: 35226400
I0315 05:44:18.052640 12110 layer_factory.hpp:77] Creating layer relu3
I0315 05:44:18.052651 12110 net.cpp:86] Creating Layer relu3
I0315 05:44:18.052656 12110 net.cpp:408] relu3 <- conv3
I0315 05:44:18.052664 12110 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:44:18.052835 12110 net.cpp:124] Setting up relu3
I0315 05:44:18.052845 12110 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:44:18.052850 12110 net.cpp:139] Memory required for data: 36864800
I0315 05:44:18.052853 12110 layer_factory.hpp:77] Creating layer pool3
I0315 05:44:18.052860 12110 net.cpp:86] Creating Layer pool3
I0315 05:44:18.052865 12110 net.cpp:408] pool3 <- conv3
I0315 05:44:18.052871 12110 net.cpp:382] pool3 -> pool3
I0315 05:44:18.053748 12110 net.cpp:124] Setting up pool3
I0315 05:44:18.053762 12110 net.cpp:131] Top shape: 100 64 4 4 (102400)
I0315 05:44:18.053768 12110 net.cpp:139] Memory required for data: 37274400
I0315 05:44:18.053772 12110 layer_factory.hpp:77] Creating layer ip1
I0315 05:44:18.053783 12110 net.cpp:86] Creating Layer ip1
I0315 05:44:18.053788 12110 net.cpp:408] ip1 <- pool3
I0315 05:44:18.053819 12110 net.cpp:382] ip1 -> ip1
I0315 05:44:18.055147 12110 net.cpp:124] Setting up ip1
I0315 05:44:18.055167 12110 net.cpp:131] Top shape: 100 10 (1000)
I0315 05:44:18.055171 12110 net.cpp:139] Memory required for data: 37278400
I0315 05:44:18.055183 12110 layer_factory.hpp:77] Creating layer loss
I0315 05:44:18.055194 12110 net.cpp:86] Creating Layer loss
I0315 05:44:18.055200 12110 net.cpp:408] loss <- ip1
I0315 05:44:18.055207 12110 net.cpp:408] loss <- label
I0315 05:44:18.055217 12110 net.cpp:382] loss -> loss
I0315 05:44:18.055233 12110 layer_factory.hpp:77] Creating layer loss
I0315 05:44:18.055537 12110 net.cpp:124] Setting up loss
I0315 05:44:18.055548 12110 net.cpp:131] Top shape: (1)
I0315 05:44:18.055552 12110 net.cpp:134]     with loss weight 1
I0315 05:44:18.055583 12110 net.cpp:139] Memory required for data: 37278404
I0315 05:44:18.055588 12110 net.cpp:200] loss needs backward computation.
I0315 05:44:18.055595 12110 net.cpp:200] ip1 needs backward computation.
I0315 05:44:18.055603 12110 net.cpp:200] pool3 needs backward computation.
I0315 05:44:18.055606 12110 net.cpp:200] relu3 needs backward computation.
I0315 05:44:18.055610 12110 net.cpp:200] conv3 needs backward computation.
I0315 05:44:18.055615 12110 net.cpp:200] norm2 needs backward computation.
I0315 05:44:18.055621 12110 net.cpp:200] pool2 needs backward computation.
I0315 05:44:18.055626 12110 net.cpp:200] relu2 needs backward computation.
I0315 05:44:18.055631 12110 net.cpp:200] conv2 needs backward computation.
I0315 05:44:18.055636 12110 net.cpp:200] norm1 needs backward computation.
I0315 05:44:18.055642 12110 net.cpp:200] relu1 needs backward computation.
I0315 05:44:18.055647 12110 net.cpp:200] pool1 needs backward computation.
I0315 05:44:18.055651 12110 net.cpp:200] conv1 needs backward computation.
I0315 05:44:18.055655 12110 net.cpp:202] concat_data does not need backward computation.
I0315 05:44:18.055661 12110 net.cpp:202] concat_label does not need backward computation.
I0315 05:44:18.055667 12110 net.cpp:202] cifar2 does not need backward computation.
I0315 05:44:18.055672 12110 net.cpp:202] cifar1 does not need backward computation.
I0315 05:44:18.055677 12110 net.cpp:244] This network produces output loss
I0315 05:44:18.055694 12110 net.cpp:257] Network initialization done.
I0315 05:44:18.056007 12110 solver.cpp:173] Creating test net (#0) specified by net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:44:18.056047 12110 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar1
I0315 05:44:18.056054 12110 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar2
I0315 05:44:18.056059 12110 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_label
I0315 05:44:18.056063 12110 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_data
I0315 05:44:18.056207 12110 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 10000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Python"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
  python_param {
    module: "cifar10_base"
    layer: "Accuracy"
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:44:18.056310 12110 layer_factory.hpp:77] Creating layer cifar
I0315 05:44:18.056377 12110 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0315 05:44:18.056406 12110 net.cpp:86] Creating Layer cifar
I0315 05:44:18.056414 12110 net.cpp:382] cifar -> data
I0315 05:44:18.056426 12110 net.cpp:382] cifar -> label
I0315 05:44:18.056437 12110 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:44:18.056608 12110 data_layer.cpp:45] output data size: 10000,3,32,32
I0315 05:44:18.315012 12110 net.cpp:124] Setting up cifar
I0315 05:44:18.315053 12110 net.cpp:131] Top shape: 10000 3 32 32 (30720000)
I0315 05:44:18.315062 12110 net.cpp:131] Top shape: 10000 (10000)
I0315 05:44:18.315065 12110 net.cpp:139] Memory required for data: 122920000
I0315 05:44:18.315073 12110 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0315 05:44:18.315091 12110 net.cpp:86] Creating Layer label_cifar_1_split
I0315 05:44:18.315126 12110 net.cpp:408] label_cifar_1_split <- label
I0315 05:44:18.315150 12110 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_0
I0315 05:44:18.315181 12110 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_1
I0315 05:44:18.315282 12110 net.cpp:124] Setting up label_cifar_1_split
I0315 05:44:18.315304 12110 net.cpp:131] Top shape: 10000 (10000)
I0315 05:44:18.315321 12110 net.cpp:131] Top shape: 10000 (10000)
I0315 05:44:18.315335 12110 net.cpp:139] Memory required for data: 123000000
I0315 05:44:18.315349 12110 layer_factory.hpp:77] Creating layer conv1
I0315 05:44:18.315377 12110 net.cpp:86] Creating Layer conv1
I0315 05:44:18.315395 12110 net.cpp:408] conv1 <- data
I0315 05:44:18.315413 12110 net.cpp:382] conv1 -> conv1
I0315 05:44:18.337882 12110 net.cpp:124] Setting up conv1
I0315 05:44:18.337908 12110 net.cpp:131] Top shape: 10000 32 32 32 (327680000)
I0315 05:44:18.337911 12110 net.cpp:139] Memory required for data: 1433720000
I0315 05:44:18.337924 12110 layer_factory.hpp:77] Creating layer pool1
I0315 05:44:18.337934 12110 net.cpp:86] Creating Layer pool1
I0315 05:44:18.337939 12110 net.cpp:408] pool1 <- conv1
I0315 05:44:18.337944 12110 net.cpp:382] pool1 -> pool1
I0315 05:44:18.338037 12110 net.cpp:124] Setting up pool1
I0315 05:44:18.338042 12110 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:44:18.338047 12110 net.cpp:139] Memory required for data: 1761400000
I0315 05:44:18.338049 12110 layer_factory.hpp:77] Creating layer relu1
I0315 05:44:18.338055 12110 net.cpp:86] Creating Layer relu1
I0315 05:44:18.338059 12110 net.cpp:408] relu1 <- pool1
I0315 05:44:18.338063 12110 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:44:18.338191 12110 net.cpp:124] Setting up relu1
I0315 05:44:18.338198 12110 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:44:18.338201 12110 net.cpp:139] Memory required for data: 2089080000
I0315 05:44:18.338202 12110 layer_factory.hpp:77] Creating layer norm1
I0315 05:44:18.338208 12110 net.cpp:86] Creating Layer norm1
I0315 05:44:18.338217 12110 net.cpp:408] norm1 <- pool1
I0315 05:44:18.338222 12110 net.cpp:382] norm1 -> norm1
I0315 05:44:18.339607 12110 net.cpp:124] Setting up norm1
I0315 05:44:18.339622 12110 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:44:18.339624 12110 net.cpp:139] Memory required for data: 2416760000
I0315 05:44:18.339627 12110 layer_factory.hpp:77] Creating layer conv2
I0315 05:44:18.339640 12110 net.cpp:86] Creating Layer conv2
I0315 05:44:18.339644 12110 net.cpp:408] conv2 <- norm1
I0315 05:44:18.339649 12110 net.cpp:382] conv2 -> conv2
I0315 05:44:18.340927 12110 net.cpp:124] Setting up conv2
I0315 05:44:18.340944 12110 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:44:18.340945 12110 net.cpp:139] Memory required for data: 2744440000
I0315 05:44:18.340955 12110 layer_factory.hpp:77] Creating layer relu2
I0315 05:44:18.340962 12110 net.cpp:86] Creating Layer relu2
I0315 05:44:18.340966 12110 net.cpp:408] relu2 <- conv2
I0315 05:44:18.340971 12110 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:44:18.341575 12110 net.cpp:124] Setting up relu2
I0315 05:44:18.341585 12110 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:44:18.341588 12110 net.cpp:139] Memory required for data: 3072120000
I0315 05:44:18.341590 12110 layer_factory.hpp:77] Creating layer pool2
I0315 05:44:18.341598 12110 net.cpp:86] Creating Layer pool2
I0315 05:44:18.341600 12110 net.cpp:408] pool2 <- conv2
I0315 05:44:18.341605 12110 net.cpp:382] pool2 -> pool2
I0315 05:44:18.341735 12110 net.cpp:124] Setting up pool2
I0315 05:44:18.341742 12110 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:44:18.341745 12110 net.cpp:139] Memory required for data: 3154040000
I0315 05:44:18.341747 12110 layer_factory.hpp:77] Creating layer norm2
I0315 05:44:18.341753 12110 net.cpp:86] Creating Layer norm2
I0315 05:44:18.341756 12110 net.cpp:408] norm2 <- pool2
I0315 05:44:18.341760 12110 net.cpp:382] norm2 -> norm2
I0315 05:44:18.342839 12110 net.cpp:124] Setting up norm2
I0315 05:44:18.342859 12110 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:44:18.342860 12110 net.cpp:139] Memory required for data: 3235960000
I0315 05:44:18.342864 12110 layer_factory.hpp:77] Creating layer conv3
I0315 05:44:18.342876 12110 net.cpp:86] Creating Layer conv3
I0315 05:44:18.342880 12110 net.cpp:408] conv3 <- norm2
I0315 05:44:18.342887 12110 net.cpp:382] conv3 -> conv3
I0315 05:44:18.344589 12110 net.cpp:124] Setting up conv3
I0315 05:44:18.344615 12110 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:44:18.344617 12110 net.cpp:139] Memory required for data: 3399800000
I0315 05:44:18.344630 12110 layer_factory.hpp:77] Creating layer relu3
I0315 05:44:18.344640 12110 net.cpp:86] Creating Layer relu3
I0315 05:44:18.344643 12110 net.cpp:408] relu3 <- conv3
I0315 05:44:18.344650 12110 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:44:18.344784 12110 net.cpp:124] Setting up relu3
I0315 05:44:18.344792 12110 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:44:18.344796 12110 net.cpp:139] Memory required for data: 3563640000
I0315 05:44:18.344799 12110 layer_factory.hpp:77] Creating layer pool3
I0315 05:44:18.344805 12110 net.cpp:86] Creating Layer pool3
I0315 05:44:18.344808 12110 net.cpp:408] pool3 <- conv3
I0315 05:44:18.344831 12110 net.cpp:382] pool3 -> pool3
I0315 05:44:18.345588 12110 net.cpp:124] Setting up pool3
I0315 05:44:18.345604 12110 net.cpp:131] Top shape: 10000 64 4 4 (10240000)
I0315 05:44:18.345607 12110 net.cpp:139] Memory required for data: 3604600000
I0315 05:44:18.345610 12110 layer_factory.hpp:77] Creating layer ip1
I0315 05:44:18.345618 12110 net.cpp:86] Creating Layer ip1
I0315 05:44:18.345621 12110 net.cpp:408] ip1 <- pool3
I0315 05:44:18.345628 12110 net.cpp:382] ip1 -> ip1
I0315 05:44:18.345814 12110 net.cpp:124] Setting up ip1
I0315 05:44:18.345819 12110 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:44:18.345821 12110 net.cpp:139] Memory required for data: 3605000000
I0315 05:44:18.345827 12110 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0315 05:44:18.345834 12110 net.cpp:86] Creating Layer ip1_ip1_0_split
I0315 05:44:18.345837 12110 net.cpp:408] ip1_ip1_0_split <- ip1
I0315 05:44:18.345840 12110 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0315 05:44:18.345845 12110 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0315 05:44:18.345871 12110 net.cpp:124] Setting up ip1_ip1_0_split
I0315 05:44:18.345876 12110 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:44:18.345878 12110 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:44:18.345880 12110 net.cpp:139] Memory required for data: 3605800000
I0315 05:44:18.345883 12110 layer_factory.hpp:77] Creating layer accuracy
I0315 05:44:18.403985 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:18.507824 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:18.626049 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:18.729964 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:19.155763 12110 net.cpp:86] Creating Layer accuracy
I0315 05:44:19.155783 12110 net.cpp:408] accuracy <- ip1_ip1_0_split_0
I0315 05:44:19.155791 12110 net.cpp:408] accuracy <- label_cifar_1_split_0
I0315 05:44:19.155799 12110 net.cpp:382] accuracy -> accuracy
I0315 05:44:19.156998 12110 net.cpp:124] Setting up accuracy
I0315 05:44:19.157018 12110 net.cpp:131] Top shape: 1 (1)
I0315 05:44:19.157021 12110 net.cpp:139] Memory required for data: 3605800004
I0315 05:44:19.157027 12110 layer_factory.hpp:77] Creating layer loss
I0315 05:44:19.157042 12110 net.cpp:86] Creating Layer loss
I0315 05:44:19.157045 12110 net.cpp:408] loss <- ip1_ip1_0_split_1
I0315 05:44:19.157052 12110 net.cpp:408] loss <- label_cifar_1_split_1
I0315 05:44:19.157060 12110 net.cpp:382] loss -> loss
I0315 05:44:19.157075 12110 layer_factory.hpp:77] Creating layer loss
I0315 05:44:19.157413 12110 net.cpp:124] Setting up loss
I0315 05:44:19.157423 12110 net.cpp:131] Top shape: (1)
I0315 05:44:19.157426 12110 net.cpp:134]     with loss weight 1
I0315 05:44:19.157438 12110 net.cpp:139] Memory required for data: 3605800008
I0315 05:44:19.157443 12110 net.cpp:200] loss needs backward computation.
I0315 05:44:19.157447 12110 net.cpp:202] accuracy does not need backward computation.
I0315 05:44:19.157454 12110 net.cpp:200] ip1_ip1_0_split needs backward computation.
I0315 05:44:19.157456 12110 net.cpp:200] ip1 needs backward computation.
I0315 05:44:19.157460 12110 net.cpp:200] pool3 needs backward computation.
I0315 05:44:19.157464 12110 net.cpp:200] relu3 needs backward computation.
I0315 05:44:19.157467 12110 net.cpp:200] conv3 needs backward computation.
I0315 05:44:19.157471 12110 net.cpp:200] norm2 needs backward computation.
I0315 05:44:19.157474 12110 net.cpp:200] pool2 needs backward computation.
I0315 05:44:19.157477 12110 net.cpp:200] relu2 needs backward computation.
I0315 05:44:19.157481 12110 net.cpp:200] conv2 needs backward computation.
I0315 05:44:19.157485 12110 net.cpp:200] norm1 needs backward computation.
I0315 05:44:19.157488 12110 net.cpp:200] relu1 needs backward computation.
I0315 05:44:19.157491 12110 net.cpp:200] pool1 needs backward computation.
I0315 05:44:19.157495 12110 net.cpp:200] conv1 needs backward computation.
I0315 05:44:19.157500 12110 net.cpp:202] label_cifar_1_split does not need backward computation.
I0315 05:44:19.157524 12110 net.cpp:202] cifar does not need backward computation.
I0315 05:44:19.157528 12110 net.cpp:244] This network produces output accuracy
I0315 05:44:19.157532 12110 net.cpp:244] This network produces output loss
I0315 05:44:19.157547 12110 net.cpp:257] Network initialization done.
I0315 05:44:19.157608 12110 solver.cpp:56] Solver scaffolding done.
I0315 05:44:19.157841 12110 caffe.cpp:242] Resuming from examples/cifar10/random/models/_iter_56000.solverstate.h5
I0315 05:44:19.158983 12110 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0315 05:44:19.160008 12110 caffe.cpp:248] Starting Optimization
I0315 05:44:19.160019 12110 solver.cpp:273] Solving CIFAR10_full
I0315 05:44:19.160022 12110 solver.cpp:274] Learning Rate Policy: fixed
I0315 05:44:19.160027 12110 solver.cpp:275] resume_file
I0315 05:44:19.160197 12110 solver.cpp:332] Iteration 56000, Testing net (#0)
I0315 05:44:19.160207 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:19.160209 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:19.160212 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:19.160215 12110 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:44:19.776667 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7601
I0315 05:44:19.776700 12110 solver.cpp:399]     Test net output #1: loss = 0.711287 (* 1 = 0.711287 loss)
I0315 05:44:19.792819 12110 solver.cpp:219] Iteration 56000 (88504.3 iter/s, 0.632738s/200 iters), loss = 0.3415
I0315 05:44:19.792863 12110 solver.cpp:238]     Train net output #0: loss = 0.3415 (* 1 = 0.3415 loss)
I0315 05:44:19.792871 12110 sgd_solver.cpp:105] Iteration 56000, lr = 0.0001
I0315 05:44:21.078430 12110 blocking_queue.cpp:49] Waiting for data
I0315 05:44:22.023237 12110 solver.cpp:219] Iteration 56200 (89.6748 iter/s, 2.23028s/200 iters), loss = 0.270183
I0315 05:44:22.023290 12110 solver.cpp:238]     Train net output #0: loss = 0.270183 (* 1 = 0.270183 loss)
I0315 05:44:22.023298 12110 sgd_solver.cpp:105] Iteration 56200, lr = 0.0001
I0315 05:44:22.542665 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:24.267971 12110 solver.cpp:219] Iteration 56400 (89.1032 iter/s, 2.24459s/200 iters), loss = 0.271103
I0315 05:44:24.268019 12110 solver.cpp:238]     Train net output #0: loss = 0.271103 (* 1 = 0.271103 loss)
I0315 05:44:24.268025 12110 sgd_solver.cpp:105] Iteration 56400, lr = 0.0001
I0315 05:44:25.270956 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:26.440824 12110 solver.cpp:219] Iteration 56600 (92.0508 iter/s, 2.17271s/200 iters), loss = 0.432272
I0315 05:44:26.440871 12110 solver.cpp:238]     Train net output #0: loss = 0.432272 (* 1 = 0.432272 loss)
I0315 05:44:26.440877 12110 sgd_solver.cpp:105] Iteration 56600, lr = 0.0001
I0315 05:44:28.068461 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:28.677954 12110 solver.cpp:219] Iteration 56800 (89.406 iter/s, 2.23699s/200 iters), loss = 0.259528
I0315 05:44:28.678007 12110 solver.cpp:238]     Train net output #0: loss = 0.259528 (* 1 = 0.259528 loss)
I0315 05:44:28.678014 12110 sgd_solver.cpp:105] Iteration 56800, lr = 0.0001
I0315 05:44:30.878746 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:30.878886 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:30.911612 12110 solver.cpp:332] Iteration 57000, Testing net (#0)
I0315 05:44:30.911639 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:30.911643 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:30.911645 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:30.911648 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:44:31.016271 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:44:31.547166 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7805
I0315 05:44:31.547196 12110 solver.cpp:399]     Test net output #1: loss = 0.6555 (* 1 = 0.6555 loss)
I0315 05:44:31.557307 12110 solver.cpp:219] Iteration 57000 (69.464 iter/s, 2.87919s/200 iters), loss = 0.333939
I0315 05:44:31.557370 12110 solver.cpp:238]     Train net output #0: loss = 0.333939 (* 1 = 0.333939 loss)
I0315 05:44:31.557377 12110 sgd_solver.cpp:105] Iteration 57000, lr = 0.0001
I0315 05:44:33.784620 12110 solver.cpp:219] Iteration 57200 (89.8006 iter/s, 2.22716s/200 iters), loss = 0.275351
I0315 05:44:33.784663 12110 solver.cpp:238]     Train net output #0: loss = 0.275351 (* 1 = 0.275351 loss)
I0315 05:44:33.784669 12110 sgd_solver.cpp:105] Iteration 57200, lr = 0.0001
I0315 05:44:34.297521 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:35.960047 12110 solver.cpp:219] Iteration 57400 (91.9417 iter/s, 2.17529s/200 iters), loss = 0.278746
I0315 05:44:35.960105 12110 solver.cpp:238]     Train net output #0: loss = 0.278746 (* 1 = 0.278746 loss)
I0315 05:44:35.960114 12110 sgd_solver.cpp:105] Iteration 57400, lr = 0.0001
I0315 05:44:37.039775 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:38.208312 12110 solver.cpp:219] Iteration 57600 (88.9636 iter/s, 2.24811s/200 iters), loss = 0.430218
I0315 05:44:38.208391 12110 solver.cpp:238]     Train net output #0: loss = 0.430218 (* 1 = 0.430218 loss)
I0315 05:44:38.208400 12110 sgd_solver.cpp:105] Iteration 57600, lr = 0.0001
I0315 05:44:39.841346 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:40.447880 12110 solver.cpp:219] Iteration 57800 (89.3096 iter/s, 2.2394s/200 iters), loss = 0.253509
I0315 05:44:40.447926 12110 solver.cpp:238]     Train net output #0: loss = 0.253509 (* 1 = 0.253509 loss)
I0315 05:44:40.447932 12110 sgd_solver.cpp:105] Iteration 57800, lr = 0.0001
I0315 05:44:42.575356 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:42.575601 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:42.615785 12110 solver.cpp:332] Iteration 58000, Testing net (#0)
I0315 05:44:42.615816 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:42.615820 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:42.615825 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:42.615828 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:44:42.754832 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:44:43.263422 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7799
I0315 05:44:43.263463 12110 solver.cpp:399]     Test net output #1: loss = 0.658086 (* 1 = 0.658086 loss)
I0315 05:44:43.297066 12110 solver.cpp:219] Iteration 58000 (70.1995 iter/s, 2.84902s/200 iters), loss = 0.333552
I0315 05:44:43.297111 12110 solver.cpp:238]     Train net output #0: loss = 0.333552 (* 1 = 0.333552 loss)
I0315 05:44:43.297118 12110 sgd_solver.cpp:105] Iteration 58000, lr = 0.0001
I0315 05:44:45.496948 12110 solver.cpp:219] Iteration 58200 (91.0073 iter/s, 2.19763s/200 iters), loss = 0.276321
I0315 05:44:45.496995 12110 solver.cpp:238]     Train net output #0: loss = 0.276321 (* 1 = 0.276321 loss)
I0315 05:44:45.497001 12110 sgd_solver.cpp:105] Iteration 58200, lr = 0.0001
I0315 05:44:46.005162 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:47.735985 12110 solver.cpp:219] Iteration 58400 (89.3299 iter/s, 2.23889s/200 iters), loss = 0.278093
I0315 05:44:47.736094 12110 solver.cpp:238]     Train net output #0: loss = 0.278093 (* 1 = 0.278093 loss)
I0315 05:44:47.736104 12110 sgd_solver.cpp:105] Iteration 58400, lr = 0.0001
I0315 05:44:48.811134 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:49.984350 12110 solver.cpp:219] Iteration 58600 (88.9614 iter/s, 2.24817s/200 iters), loss = 0.427856
I0315 05:44:49.984393 12110 solver.cpp:238]     Train net output #0: loss = 0.427856 (* 1 = 0.427856 loss)
I0315 05:44:49.984400 12110 sgd_solver.cpp:105] Iteration 58600, lr = 0.0001
I0315 05:44:51.581782 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:52.169214 12110 solver.cpp:219] Iteration 58800 (91.5448 iter/s, 2.18472s/200 iters), loss = 0.250408
I0315 05:44:52.169271 12110 solver.cpp:238]     Train net output #0: loss = 0.250408 (* 1 = 0.250408 loss)
I0315 05:44:52.169279 12110 sgd_solver.cpp:105] Iteration 58800, lr = 0.0001
I0315 05:44:54.352607 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:54.352982 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:54.386370 12110 solver.cpp:332] Iteration 59000, Testing net (#0)
I0315 05:44:54.386389 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:44:54.386392 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:44:54.386432 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:44:54.386435 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:44:54.502156 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:44:55.046417 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7806
I0315 05:44:55.046461 12110 solver.cpp:399]     Test net output #1: loss = 0.65897 (* 1 = 0.65897 loss)
I0315 05:44:55.057760 12110 solver.cpp:219] Iteration 59000 (69.2431 iter/s, 2.88838s/200 iters), loss = 0.331451
I0315 05:44:55.057799 12110 solver.cpp:238]     Train net output #0: loss = 0.331451 (* 1 = 0.331451 loss)
I0315 05:44:55.057806 12110 sgd_solver.cpp:105] Iteration 59000, lr = 0.0001
I0315 05:44:57.274724 12110 solver.cpp:219] Iteration 59200 (90.2232 iter/s, 2.21672s/200 iters), loss = 0.276102
I0315 05:44:57.274783 12110 solver.cpp:238]     Train net output #0: loss = 0.276102 (* 1 = 0.276102 loss)
I0315 05:44:57.274791 12110 sgd_solver.cpp:105] Iteration 59200, lr = 0.0001
I0315 05:44:57.782316 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:44:59.504276 12110 solver.cpp:219] Iteration 59400 (89.7103 iter/s, 2.2294s/200 iters), loss = 0.277245
I0315 05:44:59.504321 12110 solver.cpp:238]     Train net output #0: loss = 0.277245 (* 1 = 0.277245 loss)
I0315 05:44:59.504328 12110 sgd_solver.cpp:105] Iteration 59400, lr = 0.0001
I0315 05:45:00.581197 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:01.758651 12110 solver.cpp:219] Iteration 59600 (88.722 iter/s, 2.25423s/200 iters), loss = 0.426177
I0315 05:45:01.758836 12110 solver.cpp:238]     Train net output #0: loss = 0.426177 (* 1 = 0.426177 loss)
I0315 05:45:01.758878 12110 sgd_solver.cpp:105] Iteration 59600, lr = 0.0001
I0315 05:45:03.392139 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:04.000110 12110 solver.cpp:219] Iteration 59800 (89.2383 iter/s, 2.24119s/200 iters), loss = 0.248448
I0315 05:45:04.000151 12110 solver.cpp:238]     Train net output #0: loss = 0.248448 (* 1 = 0.248448 loss)
I0315 05:45:04.000156 12110 sgd_solver.cpp:105] Iteration 59800, lr = 0.0001
I0315 05:45:06.136751 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:06.137347 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:06.170330 12110 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_60000.caffemodel.h5
I0315 05:45:06.176442 12110 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_60000.solverstate.h5
I0315 05:45:06.177338 12110 solver.cpp:332] Iteration 60000, Testing net (#0)
I0315 05:45:06.177350 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:45:06.179050 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:45:06.179056 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:45:06.179059 12110 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:45:06.820956 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7808
I0315 05:45:06.820981 12110 solver.cpp:399]     Test net output #1: loss = 0.659286 (* 1 = 0.659286 loss)
I0315 05:45:06.832490 12110 solver.cpp:219] Iteration 60000 (70.616 iter/s, 2.83222s/200 iters), loss = 0.329184
I0315 05:45:06.832546 12110 solver.cpp:238]     Train net output #0: loss = 0.329184 (* 1 = 0.329184 loss)
I0315 05:45:06.832557 12110 sgd_solver.cpp:105] Iteration 60000, lr = 0.0001
I0315 05:45:06.895956 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:09.067010 12110 solver.cpp:219] Iteration 60200 (89.5107 iter/s, 2.23437s/200 iters), loss = 0.274749
I0315 05:45:09.067070 12110 solver.cpp:238]     Train net output #0: loss = 0.274749 (* 1 = 0.274749 loss)
I0315 05:45:09.067080 12110 sgd_solver.cpp:105] Iteration 60200, lr = 0.0001
I0315 05:45:09.576903 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:11.304388 12110 solver.cpp:219] Iteration 60400 (89.3967 iter/s, 2.23722s/200 iters), loss = 0.275853
I0315 05:45:11.304450 12110 solver.cpp:238]     Train net output #0: loss = 0.275853 (* 1 = 0.275853 loss)
I0315 05:45:11.304458 12110 sgd_solver.cpp:105] Iteration 60400, lr = 0.0001
I0315 05:45:12.381263 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:13.532289 12110 solver.cpp:219] Iteration 60600 (89.7769 iter/s, 2.22774s/200 iters), loss = 0.425301
I0315 05:45:13.532351 12110 solver.cpp:238]     Train net output #0: loss = 0.425301 (* 1 = 0.425301 loss)
I0315 05:45:13.532358 12110 sgd_solver.cpp:105] Iteration 60600, lr = 0.0001
I0315 05:45:15.166527 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:15.782119 12110 solver.cpp:219] Iteration 60800 (88.9016 iter/s, 2.24968s/200 iters), loss = 0.246974
I0315 05:45:15.782166 12110 solver.cpp:238]     Train net output #0: loss = 0.246974 (* 1 = 0.246974 loss)
I0315 05:45:15.782172 12110 sgd_solver.cpp:105] Iteration 60800, lr = 0.0001
I0315 05:45:17.904026 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:17.904239 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:17.937793 12110 solver.cpp:332] Iteration 61000, Testing net (#0)
I0315 05:45:17.937814 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:45:17.937816 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:45:17.937819 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:45:17.937820 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:45:18.055200 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:45:18.581795 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7808
I0315 05:45:18.581821 12110 solver.cpp:399]     Test net output #1: loss = 0.659544 (* 1 = 0.659544 loss)
I0315 05:45:18.591459 12110 solver.cpp:219] Iteration 61000 (71.1951 iter/s, 2.80918s/200 iters), loss = 0.327094
I0315 05:45:18.591505 12110 solver.cpp:238]     Train net output #0: loss = 0.327094 (* 1 = 0.327094 loss)
I0315 05:45:18.591511 12110 sgd_solver.cpp:105] Iteration 61000, lr = 0.0001
I0315 05:45:20.835923 12110 solver.cpp:219] Iteration 61200 (89.1136 iter/s, 2.24433s/200 iters), loss = 0.273994
I0315 05:45:20.835971 12110 solver.cpp:238]     Train net output #0: loss = 0.273994 (* 1 = 0.273994 loss)
I0315 05:45:20.835978 12110 sgd_solver.cpp:105] Iteration 61200, lr = 0.0001
I0315 05:45:21.332986 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:23.078125 12110 solver.cpp:219] Iteration 61400 (89.2038 iter/s, 2.24206s/200 iters), loss = 0.274475
I0315 05:45:23.078179 12110 solver.cpp:238]     Train net output #0: loss = 0.274475 (* 1 = 0.274475 loss)
I0315 05:45:23.078187 12110 sgd_solver.cpp:105] Iteration 61400, lr = 0.0001
I0315 05:45:24.141778 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:25.313889 12110 solver.cpp:219] Iteration 61600 (89.461 iter/s, 2.23561s/200 iters), loss = 0.424607
I0315 05:45:25.313944 12110 solver.cpp:238]     Train net output #0: loss = 0.424607 (* 1 = 0.424607 loss)
I0315 05:45:25.313952 12110 sgd_solver.cpp:105] Iteration 61600, lr = 0.0001
I0315 05:45:26.889017 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:27.560094 12110 solver.cpp:219] Iteration 61800 (89.0446 iter/s, 2.24606s/200 iters), loss = 0.245606
I0315 05:45:27.560129 12110 solver.cpp:238]     Train net output #0: loss = 0.245606 (* 1 = 0.245606 loss)
I0315 05:45:27.560137 12110 sgd_solver.cpp:105] Iteration 61800, lr = 0.0001
I0315 05:45:29.692203 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:29.692978 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:29.726133 12110 solver.cpp:332] Iteration 62000, Testing net (#0)
I0315 05:45:29.726151 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:45:29.726155 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:45:29.726156 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:45:29.726157 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:45:29.874238 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:45:30.356674 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7802
I0315 05:45:30.356700 12110 solver.cpp:399]     Test net output #1: loss = 0.659804 (* 1 = 0.659804 loss)
I0315 05:45:30.366886 12110 solver.cpp:219] Iteration 62000 (71.2596 iter/s, 2.80664s/200 iters), loss = 0.325736
I0315 05:45:30.366953 12110 solver.cpp:238]     Train net output #0: loss = 0.325736 (* 1 = 0.325736 loss)
I0315 05:45:30.366961 12110 sgd_solver.cpp:105] Iteration 62000, lr = 0.0001
I0315 05:45:32.617702 12110 solver.cpp:219] Iteration 62200 (88.8628 iter/s, 2.25066s/200 iters), loss = 0.273466
I0315 05:45:32.617738 12110 solver.cpp:238]     Train net output #0: loss = 0.273466 (* 1 = 0.273466 loss)
I0315 05:45:32.617743 12110 sgd_solver.cpp:105] Iteration 62200, lr = 0.0001
I0315 05:45:33.122685 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:34.841966 12110 solver.cpp:219] Iteration 62400 (89.923 iter/s, 2.22413s/200 iters), loss = 0.273642
I0315 05:45:34.842074 12110 solver.cpp:238]     Train net output #0: loss = 0.273642 (* 1 = 0.273642 loss)
I0315 05:45:34.842084 12110 sgd_solver.cpp:105] Iteration 62400, lr = 0.0001
I0315 05:45:35.917558 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:37.082281 12110 solver.cpp:219] Iteration 62600 (89.2813 iter/s, 2.24011s/200 iters), loss = 0.423939
I0315 05:45:37.082345 12110 solver.cpp:238]     Train net output #0: loss = 0.423939 (* 1 = 0.423939 loss)
I0315 05:45:37.082355 12110 sgd_solver.cpp:105] Iteration 62600, lr = 0.0001
I0315 05:45:38.720963 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:39.326355 12110 solver.cpp:219] Iteration 62800 (89.1299 iter/s, 2.24392s/200 iters), loss = 0.244404
I0315 05:45:39.326423 12110 solver.cpp:238]     Train net output #0: loss = 0.244404 (* 1 = 0.244404 loss)
I0315 05:45:39.326432 12110 sgd_solver.cpp:105] Iteration 62800, lr = 0.0001
I0315 05:45:41.454994 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:41.455430 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:41.491783 12110 solver.cpp:332] Iteration 63000, Testing net (#0)
I0315 05:45:41.491883 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:45:41.491901 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:45:41.491914 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:45:41.491925 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:45:41.582945 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:45:42.138420 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7804
I0315 05:45:42.138463 12110 solver.cpp:399]     Test net output #1: loss = 0.659978 (* 1 = 0.659978 loss)
I0315 05:45:42.152752 12110 solver.cpp:219] Iteration 63000 (70.7659 iter/s, 2.82622s/200 iters), loss = 0.324205
I0315 05:45:42.152794 12110 solver.cpp:238]     Train net output #0: loss = 0.324205 (* 1 = 0.324205 loss)
I0315 05:45:42.152801 12110 sgd_solver.cpp:105] Iteration 63000, lr = 0.0001
I0315 05:45:44.364995 12110 solver.cpp:219] Iteration 63200 (90.4983 iter/s, 2.20999s/200 iters), loss = 0.272414
I0315 05:45:44.365042 12110 solver.cpp:238]     Train net output #0: loss = 0.272414 (* 1 = 0.272414 loss)
I0315 05:45:44.365049 12110 sgd_solver.cpp:105] Iteration 63200, lr = 0.0001
I0315 05:45:44.873735 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:46.599079 12110 solver.cpp:219] Iteration 63400 (89.5277 iter/s, 2.23395s/200 iters), loss = 0.272446
I0315 05:45:46.599128 12110 solver.cpp:238]     Train net output #0: loss = 0.272446 (* 1 = 0.272446 loss)
I0315 05:45:46.599134 12110 sgd_solver.cpp:105] Iteration 63400, lr = 0.0001
I0315 05:45:47.659164 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:48.826967 12110 solver.cpp:219] Iteration 63600 (89.7769 iter/s, 2.22774s/200 iters), loss = 0.423174
I0315 05:45:48.831694 12110 solver.cpp:238]     Train net output #0: loss = 0.423174 (* 1 = 0.423174 loss)
I0315 05:45:48.831707 12110 sgd_solver.cpp:105] Iteration 63600, lr = 0.0001
I0315 05:45:50.452241 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:51.065452 12110 solver.cpp:219] Iteration 63800 (89.5383 iter/s, 2.23368s/200 iters), loss = 0.243461
I0315 05:45:51.065500 12110 solver.cpp:238]     Train net output #0: loss = 0.243461 (* 1 = 0.243461 loss)
I0315 05:45:51.065506 12110 sgd_solver.cpp:105] Iteration 63800, lr = 0.0001
I0315 05:45:53.185240 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:53.185641 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:53.219666 12110 solver.cpp:332] Iteration 64000, Testing net (#0)
I0315 05:45:53.219736 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:45:53.219749 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:45:53.219758 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:45:53.219768 12110 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:45:53.878080 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7805
I0315 05:45:53.878132 12110 solver.cpp:399]     Test net output #1: loss = 0.66002 (* 1 = 0.66002 loss)
I0315 05:45:53.906296 12110 solver.cpp:219] Iteration 64000 (70.4057 iter/s, 2.84068s/200 iters), loss = 0.3227
I0315 05:45:53.906358 12110 solver.cpp:238]     Train net output #0: loss = 0.3227 (* 1 = 0.3227 loss)
I0315 05:45:53.906366 12110 sgd_solver.cpp:105] Iteration 64000, lr = 0.0001
I0315 05:45:53.943979 12160 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:56.122918 12110 solver.cpp:219] Iteration 64200 (90.3191 iter/s, 2.21437s/200 iters), loss = 0.271641
I0315 05:45:56.122968 12110 solver.cpp:238]     Train net output #0: loss = 0.271641 (* 1 = 0.271641 loss)
I0315 05:45:56.122975 12110 sgd_solver.cpp:105] Iteration 64200, lr = 0.0001
I0315 05:45:56.635390 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:45:58.352115 12110 solver.cpp:219] Iteration 64400 (89.7241 iter/s, 2.22906s/200 iters), loss = 0.271353
I0315 05:45:58.352164 12110 solver.cpp:238]     Train net output #0: loss = 0.271353 (* 1 = 0.271353 loss)
I0315 05:45:58.352169 12110 sgd_solver.cpp:105] Iteration 64400, lr = 0.0001
I0315 05:45:59.421878 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:00.592376 12110 solver.cpp:219] Iteration 64600 (89.2809 iter/s, 2.24012s/200 iters), loss = 0.422427
I0315 05:46:00.592422 12110 solver.cpp:238]     Train net output #0: loss = 0.422427 (* 1 = 0.422427 loss)
I0315 05:46:00.592429 12110 sgd_solver.cpp:105] Iteration 64600, lr = 0.0001
I0315 05:46:02.233731 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:02.840241 12110 solver.cpp:219] Iteration 64800 (88.9789 iter/s, 2.24772s/200 iters), loss = 0.242687
I0315 05:46:02.840293 12110 solver.cpp:238]     Train net output #0: loss = 0.242687 (* 1 = 0.242687 loss)
I0315 05:46:02.840301 12110 sgd_solver.cpp:105] Iteration 64800, lr = 0.0001
I0315 05:46:05.011951 12138 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:05.012404 12137 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:05.060806 12110 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_65000.caffemodel.h5
I0315 05:46:05.067466 12110 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_65000.solverstate.h5
I0315 05:46:05.072502 12110 solver.cpp:312] Iteration 65000, loss = 0.321189
I0315 05:46:05.072533 12110 solver.cpp:332] Iteration 65000, Testing net (#0)
I0315 05:46:05.072537 12110 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:05.072540 12110 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:05.072541 12110 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:05.072542 12110 net.cpp:678] Ignoring source layer concat_data
I0315 05:46:05.206118 12160 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:46:05.702806 12110 solver.cpp:399]     Test net output #0: accuracy = 0.7802
I0315 05:46:05.702841 12110 solver.cpp:399]     Test net output #1: loss = 0.660059 (* 1 = 0.660059 loss)
I0315 05:46:05.702844 12110 solver.cpp:317] Optimization Done.
I0315 05:46:05.702847 12110 caffe.cpp:259] Optimization Done.
I0315 05:46:05.913599 14968 caffe.cpp:218] Using GPUs 2
I0315 05:46:05.936076 14968 caffe.cpp:223] GPU 2: TITAN X (Pascal)
I0315 05:46:06.275533 14968 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1
test_interval: 1000
base_lr: 2e-05
display: 200
max_iter: 70000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/random/models/"
solver_mode: GPU
device_id: 2
net: "examples/cifar10/random/cifar10_full_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_format: HDF5
I0315 05:46:06.275679 14968 solver.cpp:87] Creating training net from net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:46:06.276165 14968 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0315 05:46:06.276183 14968 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0315 05:46:06.276324 14968 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar1"
  type: "Data"
  top: "data_fixed"
  top: "label_fixed"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_20000"
    batch_size: 80
    backend: LMDB
  }
}
layer {
  name: "cifar2"
  type: "Data"
  top: "data_random"
  top: "label_random"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_random_10000"
    batch_size: 20
    backend: LMDB
  }
}
layer {
  name: "concat_label"
  type: "Concat"
  bottom: "label_fixed"
  bottom: "label_random"
  top: "label"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "concat_data"
  type: "Concat"
  bottom: "data_fixed"
  bottom: "data_random"
  top: "data"
  include {
    phase: TRAIN
  }
  concat_param {
    concat_dim: 0
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:46:06.276429 14968 layer_factory.hpp:77] Creating layer cifar1
I0315 05:46:06.276521 14968 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_20000
I0315 05:46:06.276553 14968 net.cpp:86] Creating Layer cifar1
I0315 05:46:06.276561 14968 net.cpp:382] cifar1 -> data_fixed
I0315 05:46:06.276579 14968 net.cpp:382] cifar1 -> label_fixed
I0315 05:46:06.276589 14968 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:46:06.278774 14968 data_layer.cpp:45] output data size: 80,3,32,32
I0315 05:46:06.283525 14968 net.cpp:124] Setting up cifar1
I0315 05:46:06.283551 14968 net.cpp:131] Top shape: 80 3 32 32 (245760)
I0315 05:46:06.283556 14968 net.cpp:131] Top shape: 80 (80)
I0315 05:46:06.283560 14968 net.cpp:139] Memory required for data: 983360
I0315 05:46:06.283570 14968 layer_factory.hpp:77] Creating layer cifar2
I0315 05:46:06.283664 14968 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_random_10000
I0315 05:46:06.283701 14968 net.cpp:86] Creating Layer cifar2
I0315 05:46:06.283721 14968 net.cpp:382] cifar2 -> data_random
I0315 05:46:06.283748 14968 net.cpp:382] cifar2 -> label_random
I0315 05:46:06.283768 14968 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:46:06.283938 14968 data_layer.cpp:45] output data size: 20,3,32,32
I0315 05:46:06.286351 14968 net.cpp:124] Setting up cifar2
I0315 05:46:06.286437 14968 net.cpp:131] Top shape: 20 3 32 32 (61440)
I0315 05:46:06.286455 14968 net.cpp:131] Top shape: 20 (20)
I0315 05:46:06.286466 14968 net.cpp:139] Memory required for data: 1229200
I0315 05:46:06.286478 14968 layer_factory.hpp:77] Creating layer concat_label
I0315 05:46:06.287379 14968 net.cpp:86] Creating Layer concat_label
I0315 05:46:06.287405 14968 net.cpp:408] concat_label <- label_fixed
I0315 05:46:06.287427 14968 net.cpp:408] concat_label <- label_random
I0315 05:46:06.287446 14968 net.cpp:382] concat_label -> label
I0315 05:46:06.287564 14968 net.cpp:124] Setting up concat_label
I0315 05:46:06.287581 14968 net.cpp:131] Top shape: 100 (100)
I0315 05:46:06.287591 14968 net.cpp:139] Memory required for data: 1229600
I0315 05:46:06.287602 14968 layer_factory.hpp:77] Creating layer concat_data
I0315 05:46:06.287616 14968 net.cpp:86] Creating Layer concat_data
I0315 05:46:06.287626 14968 net.cpp:408] concat_data <- data_fixed
I0315 05:46:06.287636 14968 net.cpp:408] concat_data <- data_random
I0315 05:46:06.287649 14968 net.cpp:382] concat_data -> data
I0315 05:46:06.287685 14968 net.cpp:124] Setting up concat_data
I0315 05:46:06.287703 14968 net.cpp:131] Top shape: 100 3 32 32 (307200)
I0315 05:46:06.287714 14968 net.cpp:139] Memory required for data: 2458400
I0315 05:46:06.287724 14968 layer_factory.hpp:77] Creating layer conv1
I0315 05:46:06.287750 14968 net.cpp:86] Creating Layer conv1
I0315 05:46:06.287760 14968 net.cpp:408] conv1 <- data
I0315 05:46:06.287773 14968 net.cpp:382] conv1 -> conv1
I0315 05:46:06.685458 14968 net.cpp:124] Setting up conv1
I0315 05:46:06.685483 14968 net.cpp:131] Top shape: 100 32 32 32 (3276800)
I0315 05:46:06.685487 14968 net.cpp:139] Memory required for data: 15565600
I0315 05:46:06.685508 14968 layer_factory.hpp:77] Creating layer pool1
I0315 05:46:06.685518 14968 net.cpp:86] Creating Layer pool1
I0315 05:46:06.685523 14968 net.cpp:408] pool1 <- conv1
I0315 05:46:06.685528 14968 net.cpp:382] pool1 -> pool1
I0315 05:46:06.685587 14968 net.cpp:124] Setting up pool1
I0315 05:46:06.685595 14968 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:46:06.685596 14968 net.cpp:139] Memory required for data: 18842400
I0315 05:46:06.685598 14968 layer_factory.hpp:77] Creating layer relu1
I0315 05:46:06.685602 14968 net.cpp:86] Creating Layer relu1
I0315 05:46:06.685606 14968 net.cpp:408] relu1 <- pool1
I0315 05:46:06.685631 14968 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:46:06.685786 14968 net.cpp:124] Setting up relu1
I0315 05:46:06.685793 14968 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:46:06.685796 14968 net.cpp:139] Memory required for data: 22119200
I0315 05:46:06.685797 14968 layer_factory.hpp:77] Creating layer norm1
I0315 05:46:06.685806 14968 net.cpp:86] Creating Layer norm1
I0315 05:46:06.685809 14968 net.cpp:408] norm1 <- pool1
I0315 05:46:06.685813 14968 net.cpp:382] norm1 -> norm1
I0315 05:46:06.688107 14968 net.cpp:124] Setting up norm1
I0315 05:46:06.688130 14968 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:46:06.688133 14968 net.cpp:139] Memory required for data: 25396000
I0315 05:46:06.688138 14968 layer_factory.hpp:77] Creating layer conv2
I0315 05:46:06.688153 14968 net.cpp:86] Creating Layer conv2
I0315 05:46:06.688156 14968 net.cpp:408] conv2 <- norm1
I0315 05:46:06.688164 14968 net.cpp:382] conv2 -> conv2
I0315 05:46:06.692196 14968 net.cpp:124] Setting up conv2
I0315 05:46:06.692221 14968 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:46:06.692224 14968 net.cpp:139] Memory required for data: 28672800
I0315 05:46:06.692239 14968 layer_factory.hpp:77] Creating layer relu2
I0315 05:46:06.692247 14968 net.cpp:86] Creating Layer relu2
I0315 05:46:06.692252 14968 net.cpp:408] relu2 <- conv2
I0315 05:46:06.692260 14968 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:46:06.692993 14968 net.cpp:124] Setting up relu2
I0315 05:46:06.693006 14968 net.cpp:131] Top shape: 100 32 16 16 (819200)
I0315 05:46:06.693007 14968 net.cpp:139] Memory required for data: 31949600
I0315 05:46:06.693011 14968 layer_factory.hpp:77] Creating layer pool2
I0315 05:46:06.693017 14968 net.cpp:86] Creating Layer pool2
I0315 05:46:06.693022 14968 net.cpp:408] pool2 <- conv2
I0315 05:46:06.693027 14968 net.cpp:382] pool2 -> pool2
I0315 05:46:06.693168 14968 net.cpp:124] Setting up pool2
I0315 05:46:06.693174 14968 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:46:06.693176 14968 net.cpp:139] Memory required for data: 32768800
I0315 05:46:06.693178 14968 layer_factory.hpp:77] Creating layer norm2
I0315 05:46:06.693184 14968 net.cpp:86] Creating Layer norm2
I0315 05:46:06.693187 14968 net.cpp:408] norm2 <- pool2
I0315 05:46:06.693192 14968 net.cpp:382] norm2 -> norm2
I0315 05:46:06.693382 14968 net.cpp:124] Setting up norm2
I0315 05:46:06.693388 14968 net.cpp:131] Top shape: 100 32 8 8 (204800)
I0315 05:46:06.693390 14968 net.cpp:139] Memory required for data: 33588000
I0315 05:46:06.693392 14968 layer_factory.hpp:77] Creating layer conv3
I0315 05:46:06.693401 14968 net.cpp:86] Creating Layer conv3
I0315 05:46:06.693404 14968 net.cpp:408] conv3 <- norm2
I0315 05:46:06.693408 14968 net.cpp:382] conv3 -> conv3
I0315 05:46:06.696552 14968 net.cpp:124] Setting up conv3
I0315 05:46:06.696576 14968 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:46:06.696579 14968 net.cpp:139] Memory required for data: 35226400
I0315 05:46:06.696591 14968 layer_factory.hpp:77] Creating layer relu3
I0315 05:46:06.696599 14968 net.cpp:86] Creating Layer relu3
I0315 05:46:06.696604 14968 net.cpp:408] relu3 <- conv3
I0315 05:46:06.696607 14968 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:46:06.696727 14968 net.cpp:124] Setting up relu3
I0315 05:46:06.696733 14968 net.cpp:131] Top shape: 100 64 8 8 (409600)
I0315 05:46:06.696734 14968 net.cpp:139] Memory required for data: 36864800
I0315 05:46:06.696737 14968 layer_factory.hpp:77] Creating layer pool3
I0315 05:46:06.696741 14968 net.cpp:86] Creating Layer pool3
I0315 05:46:06.696743 14968 net.cpp:408] pool3 <- conv3
I0315 05:46:06.696748 14968 net.cpp:382] pool3 -> pool3
I0315 05:46:06.697500 14968 net.cpp:124] Setting up pool3
I0315 05:46:06.697510 14968 net.cpp:131] Top shape: 100 64 4 4 (102400)
I0315 05:46:06.697513 14968 net.cpp:139] Memory required for data: 37274400
I0315 05:46:06.697515 14968 layer_factory.hpp:77] Creating layer ip1
I0315 05:46:06.697522 14968 net.cpp:86] Creating Layer ip1
I0315 05:46:06.697525 14968 net.cpp:408] ip1 <- pool3
I0315 05:46:06.697547 14968 net.cpp:382] ip1 -> ip1
I0315 05:46:06.698704 14968 net.cpp:124] Setting up ip1
I0315 05:46:06.698717 14968 net.cpp:131] Top shape: 100 10 (1000)
I0315 05:46:06.698719 14968 net.cpp:139] Memory required for data: 37278400
I0315 05:46:06.698726 14968 layer_factory.hpp:77] Creating layer loss
I0315 05:46:06.698734 14968 net.cpp:86] Creating Layer loss
I0315 05:46:06.698738 14968 net.cpp:408] loss <- ip1
I0315 05:46:06.698742 14968 net.cpp:408] loss <- label
I0315 05:46:06.698748 14968 net.cpp:382] loss -> loss
I0315 05:46:06.698758 14968 layer_factory.hpp:77] Creating layer loss
I0315 05:46:06.698964 14968 net.cpp:124] Setting up loss
I0315 05:46:06.698971 14968 net.cpp:131] Top shape: (1)
I0315 05:46:06.698972 14968 net.cpp:134]     with loss weight 1
I0315 05:46:06.698990 14968 net.cpp:139] Memory required for data: 37278404
I0315 05:46:06.698992 14968 net.cpp:200] loss needs backward computation.
I0315 05:46:06.698997 14968 net.cpp:200] ip1 needs backward computation.
I0315 05:46:06.698999 14968 net.cpp:200] pool3 needs backward computation.
I0315 05:46:06.699002 14968 net.cpp:200] relu3 needs backward computation.
I0315 05:46:06.699004 14968 net.cpp:200] conv3 needs backward computation.
I0315 05:46:06.699007 14968 net.cpp:200] norm2 needs backward computation.
I0315 05:46:06.699009 14968 net.cpp:200] pool2 needs backward computation.
I0315 05:46:06.699012 14968 net.cpp:200] relu2 needs backward computation.
I0315 05:46:06.699013 14968 net.cpp:200] conv2 needs backward computation.
I0315 05:46:06.699015 14968 net.cpp:200] norm1 needs backward computation.
I0315 05:46:06.699018 14968 net.cpp:200] relu1 needs backward computation.
I0315 05:46:06.699020 14968 net.cpp:200] pool1 needs backward computation.
I0315 05:46:06.699023 14968 net.cpp:200] conv1 needs backward computation.
I0315 05:46:06.699024 14968 net.cpp:202] concat_data does not need backward computation.
I0315 05:46:06.699028 14968 net.cpp:202] concat_label does not need backward computation.
I0315 05:46:06.699031 14968 net.cpp:202] cifar2 does not need backward computation.
I0315 05:46:06.699033 14968 net.cpp:202] cifar1 does not need backward computation.
I0315 05:46:06.699034 14968 net.cpp:244] This network produces output loss
I0315 05:46:06.699045 14968 net.cpp:257] Network initialization done.
I0315 05:46:06.699242 14968 solver.cpp:173] Creating test net (#0) specified by net file: examples/cifar10/random/cifar10_full_train_test.prototxt
I0315 05:46:06.699266 14968 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar1
I0315 05:46:06.699270 14968 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar2
I0315 05:46:06.699272 14968 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_label
I0315 05:46:06.699275 14968 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer concat_data
I0315 05:46:06.699357 14968 net.cpp:53] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 10000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Python"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
  python_param {
    module: "cifar10_base"
    layer: "Accuracy"
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0315 05:46:06.699426 14968 layer_factory.hpp:77] Creating layer cifar
I0315 05:46:06.699475 14968 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0315 05:46:06.699493 14968 net.cpp:86] Creating Layer cifar
I0315 05:46:06.699497 14968 net.cpp:382] cifar -> data
I0315 05:46:06.699503 14968 net.cpp:382] cifar -> label
I0315 05:46:06.699509 14968 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0315 05:46:06.699630 14968 data_layer.cpp:45] output data size: 10000,3,32,32
I0315 05:46:06.945526 14968 net.cpp:124] Setting up cifar
I0315 05:46:06.945550 14968 net.cpp:131] Top shape: 10000 3 32 32 (30720000)
I0315 05:46:06.945554 14968 net.cpp:131] Top shape: 10000 (10000)
I0315 05:46:06.945556 14968 net.cpp:139] Memory required for data: 122920000
I0315 05:46:06.945561 14968 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0315 05:46:06.945572 14968 net.cpp:86] Creating Layer label_cifar_1_split
I0315 05:46:06.945576 14968 net.cpp:408] label_cifar_1_split <- label
I0315 05:46:06.945581 14968 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_0
I0315 05:46:06.945590 14968 net.cpp:382] label_cifar_1_split -> label_cifar_1_split_1
I0315 05:46:06.945648 14968 net.cpp:124] Setting up label_cifar_1_split
I0315 05:46:06.945657 14968 net.cpp:131] Top shape: 10000 (10000)
I0315 05:46:06.945660 14968 net.cpp:131] Top shape: 10000 (10000)
I0315 05:46:06.945663 14968 net.cpp:139] Memory required for data: 123000000
I0315 05:46:06.945667 14968 layer_factory.hpp:77] Creating layer conv1
I0315 05:46:06.945680 14968 net.cpp:86] Creating Layer conv1
I0315 05:46:06.945686 14968 net.cpp:408] conv1 <- data
I0315 05:46:06.945693 14968 net.cpp:382] conv1 -> conv1
I0315 05:46:06.966145 14968 net.cpp:124] Setting up conv1
I0315 05:46:06.966176 14968 net.cpp:131] Top shape: 10000 32 32 32 (327680000)
I0315 05:46:06.966181 14968 net.cpp:139] Memory required for data: 1433720000
I0315 05:46:06.966197 14968 layer_factory.hpp:77] Creating layer pool1
I0315 05:46:06.966209 14968 net.cpp:86] Creating Layer pool1
I0315 05:46:06.966214 14968 net.cpp:408] pool1 <- conv1
I0315 05:46:06.966223 14968 net.cpp:382] pool1 -> pool1
I0315 05:46:06.966295 14968 net.cpp:124] Setting up pool1
I0315 05:46:06.966303 14968 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:46:06.966308 14968 net.cpp:139] Memory required for data: 1761400000
I0315 05:46:06.966311 14968 layer_factory.hpp:77] Creating layer relu1
I0315 05:46:06.966320 14968 net.cpp:86] Creating Layer relu1
I0315 05:46:06.966325 14968 net.cpp:408] relu1 <- pool1
I0315 05:46:06.966331 14968 net.cpp:369] relu1 -> pool1 (in-place)
I0315 05:46:06.966562 14968 net.cpp:124] Setting up relu1
I0315 05:46:06.966572 14968 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:46:06.966578 14968 net.cpp:139] Memory required for data: 2089080000
I0315 05:46:06.966581 14968 layer_factory.hpp:77] Creating layer norm1
I0315 05:46:06.966590 14968 net.cpp:86] Creating Layer norm1
I0315 05:46:06.966593 14968 net.cpp:408] norm1 <- pool1
I0315 05:46:06.966599 14968 net.cpp:382] norm1 -> norm1
I0315 05:46:06.971724 14968 net.cpp:124] Setting up norm1
I0315 05:46:06.971748 14968 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:46:06.971752 14968 net.cpp:139] Memory required for data: 2416760000
I0315 05:46:06.971758 14968 layer_factory.hpp:77] Creating layer conv2
I0315 05:46:06.971776 14968 net.cpp:86] Creating Layer conv2
I0315 05:46:06.971782 14968 net.cpp:408] conv2 <- norm1
I0315 05:46:06.971794 14968 net.cpp:382] conv2 -> conv2
I0315 05:46:06.973852 14968 net.cpp:124] Setting up conv2
I0315 05:46:06.973881 14968 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:46:06.973884 14968 net.cpp:139] Memory required for data: 2744440000
I0315 05:46:06.973903 14968 layer_factory.hpp:77] Creating layer relu2
I0315 05:46:06.973917 14968 net.cpp:86] Creating Layer relu2
I0315 05:46:06.973923 14968 net.cpp:408] relu2 <- conv2
I0315 05:46:06.973932 14968 net.cpp:369] relu2 -> conv2 (in-place)
I0315 05:46:06.974992 14968 net.cpp:124] Setting up relu2
I0315 05:46:06.975018 14968 net.cpp:131] Top shape: 10000 32 16 16 (81920000)
I0315 05:46:06.975023 14968 net.cpp:139] Memory required for data: 3072120000
I0315 05:46:06.975028 14968 layer_factory.hpp:77] Creating layer pool2
I0315 05:46:06.975042 14968 net.cpp:86] Creating Layer pool2
I0315 05:46:06.975047 14968 net.cpp:408] pool2 <- conv2
I0315 05:46:06.975056 14968 net.cpp:382] pool2 -> pool2
I0315 05:46:06.975277 14968 net.cpp:124] Setting up pool2
I0315 05:46:06.975288 14968 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:46:06.975293 14968 net.cpp:139] Memory required for data: 3154040000
I0315 05:46:06.975297 14968 layer_factory.hpp:77] Creating layer norm2
I0315 05:46:06.975307 14968 net.cpp:86] Creating Layer norm2
I0315 05:46:06.975313 14968 net.cpp:408] norm2 <- pool2
I0315 05:46:06.975319 14968 net.cpp:382] norm2 -> norm2
I0315 05:46:06.977079 14968 net.cpp:124] Setting up norm2
I0315 05:46:06.977102 14968 net.cpp:131] Top shape: 10000 32 8 8 (20480000)
I0315 05:46:06.977107 14968 net.cpp:139] Memory required for data: 3235960000
I0315 05:46:06.977113 14968 layer_factory.hpp:77] Creating layer conv3
I0315 05:46:06.977129 14968 net.cpp:86] Creating Layer conv3
I0315 05:46:06.977134 14968 net.cpp:408] conv3 <- norm2
I0315 05:46:06.977144 14968 net.cpp:382] conv3 -> conv3
I0315 05:46:06.979569 14968 net.cpp:124] Setting up conv3
I0315 05:46:06.979598 14968 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:46:06.979601 14968 net.cpp:139] Memory required for data: 3399800000
I0315 05:46:06.979621 14968 layer_factory.hpp:77] Creating layer relu3
I0315 05:46:06.979635 14968 net.cpp:86] Creating Layer relu3
I0315 05:46:06.979642 14968 net.cpp:408] relu3 <- conv3
I0315 05:46:06.979651 14968 net.cpp:369] relu3 -> conv3 (in-place)
I0315 05:46:06.979840 14968 net.cpp:124] Setting up relu3
I0315 05:46:06.979851 14968 net.cpp:131] Top shape: 10000 64 8 8 (40960000)
I0315 05:46:06.979856 14968 net.cpp:139] Memory required for data: 3563640000
I0315 05:46:06.979859 14968 layer_factory.hpp:77] Creating layer pool3
I0315 05:46:06.979867 14968 net.cpp:86] Creating Layer pool3
I0315 05:46:06.979871 14968 net.cpp:408] pool3 <- conv3
I0315 05:46:06.979908 14968 net.cpp:382] pool3 -> pool3
I0315 05:46:06.980963 14968 net.cpp:124] Setting up pool3
I0315 05:46:06.980983 14968 net.cpp:131] Top shape: 10000 64 4 4 (10240000)
I0315 05:46:06.980988 14968 net.cpp:139] Memory required for data: 3604600000
I0315 05:46:06.980993 14968 layer_factory.hpp:77] Creating layer ip1
I0315 05:46:06.981005 14968 net.cpp:86] Creating Layer ip1
I0315 05:46:06.981011 14968 net.cpp:408] ip1 <- pool3
I0315 05:46:06.981019 14968 net.cpp:382] ip1 -> ip1
I0315 05:46:06.981302 14968 net.cpp:124] Setting up ip1
I0315 05:46:06.981310 14968 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:46:06.981317 14968 net.cpp:139] Memory required for data: 3605000000
I0315 05:46:06.981324 14968 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0315 05:46:06.981333 14968 net.cpp:86] Creating Layer ip1_ip1_0_split
I0315 05:46:06.981335 14968 net.cpp:408] ip1_ip1_0_split <- ip1
I0315 05:46:06.981341 14968 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0315 05:46:06.981348 14968 net.cpp:382] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0315 05:46:06.981387 14968 net.cpp:124] Setting up ip1_ip1_0_split
I0315 05:46:06.981395 14968 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:46:06.981400 14968 net.cpp:131] Top shape: 10000 10 (100000)
I0315 05:46:06.981403 14968 net.cpp:139] Memory required for data: 3605800000
I0315 05:46:06.981407 14968 layer_factory.hpp:77] Creating layer accuracy
I0315 05:46:07.052459 15017 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:07.165256 15017 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:07.269580 15017 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:07.377140 15017 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:07.560328 14968 net.cpp:86] Creating Layer accuracy
I0315 05:46:07.560351 14968 net.cpp:408] accuracy <- ip1_ip1_0_split_0
I0315 05:46:07.560358 14968 net.cpp:408] accuracy <- label_cifar_1_split_0
I0315 05:46:07.560364 14968 net.cpp:382] accuracy -> accuracy
I0315 05:46:07.561352 14968 net.cpp:124] Setting up accuracy
I0315 05:46:07.561368 14968 net.cpp:131] Top shape: 1 (1)
I0315 05:46:07.561370 14968 net.cpp:139] Memory required for data: 3605800004
I0315 05:46:07.561375 14968 layer_factory.hpp:77] Creating layer loss
I0315 05:46:07.561385 14968 net.cpp:86] Creating Layer loss
I0315 05:46:07.561388 14968 net.cpp:408] loss <- ip1_ip1_0_split_1
I0315 05:46:07.561393 14968 net.cpp:408] loss <- label_cifar_1_split_1
I0315 05:46:07.561396 14968 net.cpp:382] loss -> loss
I0315 05:46:07.561408 14968 layer_factory.hpp:77] Creating layer loss
I0315 05:46:07.561745 14968 net.cpp:124] Setting up loss
I0315 05:46:07.561753 14968 net.cpp:131] Top shape: (1)
I0315 05:46:07.561754 14968 net.cpp:134]     with loss weight 1
I0315 05:46:07.561764 14968 net.cpp:139] Memory required for data: 3605800008
I0315 05:46:07.561767 14968 net.cpp:200] loss needs backward computation.
I0315 05:46:07.561770 14968 net.cpp:202] accuracy does not need backward computation.
I0315 05:46:07.561774 14968 net.cpp:200] ip1_ip1_0_split needs backward computation.
I0315 05:46:07.561775 14968 net.cpp:200] ip1 needs backward computation.
I0315 05:46:07.561777 14968 net.cpp:200] pool3 needs backward computation.
I0315 05:46:07.561781 14968 net.cpp:200] relu3 needs backward computation.
I0315 05:46:07.561784 14968 net.cpp:200] conv3 needs backward computation.
I0315 05:46:07.561786 14968 net.cpp:200] norm2 needs backward computation.
I0315 05:46:07.561789 14968 net.cpp:200] pool2 needs backward computation.
I0315 05:46:07.561789 14968 net.cpp:200] relu2 needs backward computation.
I0315 05:46:07.561794 14968 net.cpp:200] conv2 needs backward computation.
I0315 05:46:07.561795 14968 net.cpp:200] norm1 needs backward computation.
I0315 05:46:07.561797 14968 net.cpp:200] relu1 needs backward computation.
I0315 05:46:07.561800 14968 net.cpp:200] pool1 needs backward computation.
I0315 05:46:07.561801 14968 net.cpp:200] conv1 needs backward computation.
I0315 05:46:07.561805 14968 net.cpp:202] label_cifar_1_split does not need backward computation.
I0315 05:46:07.561825 14968 net.cpp:202] cifar does not need backward computation.
I0315 05:46:07.561826 14968 net.cpp:244] This network produces output accuracy
I0315 05:46:07.561830 14968 net.cpp:244] This network produces output loss
I0315 05:46:07.561841 14968 net.cpp:257] Network initialization done.
I0315 05:46:07.561897 14968 solver.cpp:56] Solver scaffolding done.
I0315 05:46:07.562127 14968 caffe.cpp:242] Resuming from examples/cifar10/random/models/_iter_65000.solverstate.h5
I0315 05:46:07.563269 14968 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0315 05:46:07.564298 14968 caffe.cpp:248] Starting Optimization
I0315 05:46:07.564311 14968 solver.cpp:273] Solving CIFAR10_full
I0315 05:46:07.564312 14968 solver.cpp:274] Learning Rate Policy: fixed
I0315 05:46:07.564314 14968 solver.cpp:275] resume_file
I0315 05:46:07.564486 14968 solver.cpp:332] Iteration 65000, Testing net (#0)
I0315 05:46:07.564493 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:07.564496 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:07.564497 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:07.564499 14968 net.cpp:678] Ignoring source layer concat_data
class 0, acc 0I0315 05:46:08.175246 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7802
I0315 05:46:08.175271 14968 solver.cpp:399]     Test net output #1: loss = 0.660059 (* 1 = 0.660059 loss)
I0315 05:46:08.186419 14968 solver.cpp:219] Iteration 65000 (-2.06776e+12 iter/s, 0.622046s/200 iters), loss = 0.321189
I0315 05:46:08.186465 14968 solver.cpp:238]     Train net output #0: loss = 0.321189 (* 1 = 0.321189 loss)
I0315 05:46:08.186472 14968 sgd_solver.cpp:105] Iteration 65000, lr = 2e-05
I0315 05:46:10.357260 14968 solver.cpp:219] Iteration 65200 (92.1363 iter/s, 2.1707s/200 iters), loss = 0.238518
I0315 05:46:10.357552 14968 solver.cpp:238]     Train net output #0: loss = 0.238518 (* 1 = 0.238518 loss)
I0315 05:46:10.357646 14968 sgd_solver.cpp:105] Iteration 65200, lr = 2e-05
I0315 05:46:10.872388 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:10.928020 14968 blocking_queue.cpp:49] Waiting for data
I0315 05:46:12.595871 14968 solver.cpp:219] Iteration 65400 (89.3564 iter/s, 2.23823s/200 iters), loss = 0.253004
I0315 05:46:12.595932 14968 solver.cpp:238]     Train net output #0: loss = 0.253004 (* 1 = 0.253004 loss)
I0315 05:46:12.595938 14968 sgd_solver.cpp:105] Iteration 65400, lr = 2e-05
I0315 05:46:13.665218 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:14.838539 14968 solver.cpp:219] Iteration 65600 (89.1854 iter/s, 2.24252s/200 iters), loss = 0.416178
I0315 05:46:14.838582 14968 solver.cpp:238]     Train net output #0: loss = 0.416178 (* 1 = 0.416178 loss)
I0315 05:46:14.838588 14968 sgd_solver.cpp:105] Iteration 65600, lr = 2e-05
I0315 05:46:16.471269 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:17.071182 14968 solver.cpp:219] Iteration 65800 (89.5855 iter/s, 2.2325s/200 iters), loss = 0.241791
I0315 05:46:17.071226 14968 solver.cpp:238]     Train net output #0: loss = 0.241791 (* 1 = 0.241791 loss)
I0315 05:46:17.071233 14968 sgd_solver.cpp:105] Iteration 65800, lr = 2e-05
I0315 05:46:19.260777 14997 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:19.261063 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:19.293503 14968 solver.cpp:332] Iteration 66000, Testing net (#0)
I0315 05:46:19.293529 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:19.293530 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:19.293532 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:19.293534 14968 net.cpp:678] Ignoring source layer concat_data
I0315 05:46:19.397143 15017 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:46:19.925448 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7932
I0315 05:46:19.925473 14968 solver.cpp:399]     Test net output #1: loss = 0.627824 (* 1 = 0.627824 loss)
I0315 05:46:19.935747 14968 solver.cpp:219] Iteration 66000 (69.8225 iter/s, 2.86441s/200 iters), loss = 0.290168
I0315 05:46:19.935817 14968 solver.cpp:238]     Train net output #0: loss = 0.290168 (* 1 = 0.290168 loss)
I0315 05:46:19.935823 14968 sgd_solver.cpp:105] Iteration 66000, lr = 2e-05
I0315 05:46:22.170426 14968 solver.cpp:219] Iteration 66200 (89.5049 iter/s, 2.23452s/200 iters), loss = 0.240633
I0315 05:46:22.170488 14968 solver.cpp:238]     Train net output #0: loss = 0.240633 (* 1 = 0.240633 loss)
I0315 05:46:22.170495 14968 sgd_solver.cpp:105] Iteration 66200, lr = 2e-05
I0315 05:46:22.629786 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:24.365850 14968 solver.cpp:219] Iteration 66400 (91.1923 iter/s, 2.19317s/200 iters), loss = 0.256091
I0315 05:46:24.365909 14968 solver.cpp:238]     Train net output #0: loss = 0.256091 (* 1 = 0.256091 loss)
I0315 05:46:24.365917 14968 sgd_solver.cpp:105] Iteration 66400, lr = 2e-05
I0315 05:46:25.426219 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:26.588508 14968 solver.cpp:219] Iteration 66600 (89.9883 iter/s, 2.22251s/200 iters), loss = 0.414178
I0315 05:46:26.588563 14968 solver.cpp:238]     Train net output #0: loss = 0.414178 (* 1 = 0.414178 loss)
I0315 05:46:26.588572 14968 sgd_solver.cpp:105] Iteration 66600, lr = 2e-05
I0315 05:46:28.223639 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:28.830772 14968 solver.cpp:219] Iteration 66800 (89.2014 iter/s, 2.24212s/200 iters), loss = 0.240324
I0315 05:46:28.830821 14968 solver.cpp:238]     Train net output #0: loss = 0.240324 (* 1 = 0.240324 loss)
I0315 05:46:28.830826 14968 sgd_solver.cpp:105] Iteration 66800, lr = 2e-05
I0315 05:46:31.038127 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:31.042325 14997 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:31.072999 14968 solver.cpp:332] Iteration 67000, Testing net (#0)
I0315 05:46:31.073024 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:31.073026 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:31.073029 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:31.073029 14968 net.cpp:678] Ignoring source layer concat_data
I0315 05:46:31.185412 15017 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:46:31.715965 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7929
I0315 05:46:31.715993 14968 solver.cpp:399]     Test net output #1: loss = 0.6277 (* 1 = 0.6277 loss)
I0315 05:46:31.726500 14968 solver.cpp:219] Iteration 67000 (69.0712 iter/s, 2.89556s/200 iters), loss = 0.289956
I0315 05:46:31.726552 14968 solver.cpp:238]     Train net output #0: loss = 0.289956 (* 1 = 0.289956 loss)
I0315 05:46:31.726559 14968 sgd_solver.cpp:105] Iteration 67000, lr = 2e-05
I0315 05:46:33.959848 14968 solver.cpp:219] Iteration 67200 (89.5573 iter/s, 2.23321s/200 iters), loss = 0.240622
I0315 05:46:33.959897 14968 solver.cpp:238]     Train net output #0: loss = 0.240622 (* 1 = 0.240622 loss)
I0315 05:46:33.959903 14968 sgd_solver.cpp:105] Iteration 67200, lr = 2e-05
I0315 05:46:34.412613 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:36.150748 14968 solver.cpp:219] Iteration 67400 (91.2926 iter/s, 2.19076s/200 iters), loss = 0.258013
I0315 05:46:36.150836 14968 solver.cpp:238]     Train net output #0: loss = 0.258013 (* 1 = 0.258013 loss)
I0315 05:46:36.150843 14968 sgd_solver.cpp:105] Iteration 67400, lr = 2e-05
I0315 05:46:37.226960 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:38.402812 14968 solver.cpp:219] Iteration 67600 (88.8146 iter/s, 2.25188s/200 iters), loss = 0.414231
I0315 05:46:38.402858 14968 solver.cpp:238]     Train net output #0: loss = 0.414231 (* 1 = 0.414231 loss)
I0315 05:46:38.402863 14968 sgd_solver.cpp:105] Iteration 67600, lr = 2e-05
I0315 05:46:40.027607 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:40.634062 14968 solver.cpp:219] Iteration 67800 (89.643 iter/s, 2.23107s/200 iters), loss = 0.239875
I0315 05:46:40.634112 14968 solver.cpp:238]     Train net output #0: loss = 0.239875 (* 1 = 0.239875 loss)
I0315 05:46:40.634117 14968 sgd_solver.cpp:105] Iteration 67800, lr = 2e-05
I0315 05:46:42.823258 14997 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:42.823524 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:42.857437 14968 solver.cpp:332] Iteration 68000, Testing net (#0)
I0315 05:46:42.857455 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:42.857457 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:42.857460 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:42.857461 14968 net.cpp:678] Ignoring source layer concat_data
I0315 05:46:42.999791 15017 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:46:43.491811 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7921
I0315 05:46:43.491837 14968 solver.cpp:399]     Test net output #1: loss = 0.627728 (* 1 = 0.627728 loss)
I0315 05:46:43.501760 14968 solver.cpp:219] Iteration 68000 (69.7463 iter/s, 2.86753s/200 iters), loss = 0.290175
I0315 05:46:43.501804 14968 solver.cpp:238]     Train net output #0: loss = 0.290175 (* 1 = 0.290175 loss)
I0315 05:46:43.501811 14968 sgd_solver.cpp:105] Iteration 68000, lr = 2e-05
I0315 05:46:45.769399 14968 solver.cpp:219] Iteration 68200 (88.2025 iter/s, 2.26751s/200 iters), loss = 0.240383
I0315 05:46:45.769434 14968 solver.cpp:238]     Train net output #0: loss = 0.240383 (* 1 = 0.240383 loss)
I0315 05:46:45.769438 14968 sgd_solver.cpp:105] Iteration 68200, lr = 2e-05
I0315 05:46:46.280153 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:47.937340 14968 solver.cpp:219] Iteration 68400 (92.2589 iter/s, 2.16781s/200 iters), loss = 0.259212
I0315 05:46:47.937387 14968 solver.cpp:238]     Train net output #0: loss = 0.259212 (* 1 = 0.259212 loss)
I0315 05:46:47.937393 14968 sgd_solver.cpp:105] Iteration 68400, lr = 2e-05
I0315 05:46:48.999900 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:50.168695 14968 solver.cpp:219] Iteration 68600 (89.6372 iter/s, 2.23122s/200 iters), loss = 0.414248
I0315 05:46:50.168745 14968 solver.cpp:238]     Train net output #0: loss = 0.414248 (* 1 = 0.414248 loss)
I0315 05:46:50.168751 14968 sgd_solver.cpp:105] Iteration 68600, lr = 2e-05
I0315 05:46:51.800634 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:52.418468 14968 solver.cpp:219] Iteration 68800 (88.9038 iter/s, 2.24962s/200 iters), loss = 0.239717
I0315 05:46:52.418520 14968 solver.cpp:238]     Train net output #0: loss = 0.239717 (* 1 = 0.239717 loss)
I0315 05:46:52.418525 14968 sgd_solver.cpp:105] Iteration 68800, lr = 2e-05
I0315 05:46:54.600679 14997 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:54.601249 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:54.634299 14968 solver.cpp:332] Iteration 69000, Testing net (#0)
I0315 05:46:54.634323 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:46:54.634325 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:46:54.634327 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:46:54.634330 14968 net.cpp:678] Ignoring source layer concat_data
I0315 05:46:54.753120 15017 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:46:55.272408 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7925
I0315 05:46:55.272447 14968 solver.cpp:399]     Test net output #1: loss = 0.627793 (* 1 = 0.627793 loss)
I0315 05:46:55.282155 14968 solver.cpp:219] Iteration 69000 (69.8441 iter/s, 2.86352s/200 iters), loss = 0.290404
I0315 05:46:55.282205 14968 solver.cpp:238]     Train net output #0: loss = 0.290404 (* 1 = 0.290404 loss)
I0315 05:46:55.282212 14968 sgd_solver.cpp:105] Iteration 69000, lr = 2e-05
I0315 05:46:57.524406 14968 solver.cpp:219] Iteration 69200 (89.2016 iter/s, 2.24211s/200 iters), loss = 0.240216
I0315 05:46:57.524451 14968 solver.cpp:238]     Train net output #0: loss = 0.240216 (* 1 = 0.240216 loss)
I0315 05:46:57.524457 14968 sgd_solver.cpp:105] Iteration 69200, lr = 2e-05
I0315 05:46:57.977386 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:46:59.706511 14968 solver.cpp:219] Iteration 69400 (91.66 iter/s, 2.18198s/200 iters), loss = 0.259912
I0315 05:46:59.706557 14968 solver.cpp:238]     Train net output #0: loss = 0.259912 (* 1 = 0.259912 loss)
I0315 05:46:59.706564 14968 sgd_solver.cpp:105] Iteration 69400, lr = 2e-05
I0315 05:47:00.788405 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:47:01.954623 14968 solver.cpp:219] Iteration 69600 (88.9819 iter/s, 2.24765s/200 iters), loss = 0.414273
I0315 05:47:01.954672 14968 solver.cpp:238]     Train net output #0: loss = 0.414273 (* 1 = 0.414273 loss)
I0315 05:47:01.954679 14968 sgd_solver.cpp:105] Iteration 69600, lr = 2e-05
I0315 05:47:03.575641 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:47:04.188107 14968 solver.cpp:219] Iteration 69800 (89.5519 iter/s, 2.23334s/200 iters), loss = 0.239547
I0315 05:47:04.188155 14968 solver.cpp:238]     Train net output #0: loss = 0.239547 (* 1 = 0.239547 loss)
I0315 05:47:04.188161 14968 sgd_solver.cpp:105] Iteration 69800, lr = 2e-05
I0315 05:47:06.383683 14997 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:47:06.384313 14996 data_layer.cpp:73] Restarting data prefetching from start.
I0315 05:47:06.416455 14968 solver.cpp:459] Snapshotting to HDF5 file examples/cifar10/random/models/_iter_70000.caffemodel.h5
I0315 05:47:06.422610 14968 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/random/models/_iter_70000.solverstate.h5
I0315 05:47:06.427335 14968 solver.cpp:312] Iteration 70000, loss = 0.290483
I0315 05:47:06.427366 14968 solver.cpp:332] Iteration 70000, Testing net (#0)
I0315 05:47:06.427369 14968 net.cpp:678] Ignoring source layer cifar1
I0315 05:47:06.427371 14968 net.cpp:678] Ignoring source layer cifar2
I0315 05:47:06.427373 14968 net.cpp:678] Ignoring source layer concat_label
I0315 05:47:06.427374 14968 net.cpp:678] Ignoring source layer concat_data
I0315 05:47:06.565811 15017 data_layer.cpp:73] Restarting data prefetching from start.
class 0, acc 0I0315 05:47:07.048300 14968 solver.cpp:399]     Test net output #0: accuracy = 0.7924
I0315 05:47:07.048327 14968 solver.cpp:399]     Test net output #1: loss = 0.627876 (* 1 = 0.627876 loss)
I0315 05:47:07.048331 14968 solver.cpp:317] Optimization Done.
I0315 05:47:07.048333 14968 caffe.cpp:259] Optimization Done.
